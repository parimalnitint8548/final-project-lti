{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13318578,"sourceType":"datasetVersion","datasetId":8443100},{"sourceId":13319186,"sourceType":"datasetVersion","datasetId":8443568},{"sourceId":604917,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":453681,"modelId":469959}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nResNet-50 Deepfake Detector - Kaggle GPU Optimized\nVERSION FOR COMBINED IMAGE (e.g., WildDeepfake) & VIDEO (e.g., FF++) DATASETS\n\"\"\"\n\nimport argparse, io, json, math, os, random, time, sys\nfrom pathlib import Path\nfrom collections import defaultdict\n\n# --- [SETUP] Install necessary libraries ---\ntry:\n    import cv2\nexcept ImportError:\n    print(\"OpenCV not found. Installing opencv-python-headless...\")\n    import subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"opencv-python-headless\"])\n    import cv2\n\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, SubsetRandomSampler, ConcatDataset\nfrom torchvision import datasets, transforms, models\n\nfrom sklearn.metrics import (\n    accuracy_score, precision_recall_fscore_support,\n    confusion_matrix, roc_auc_score, roc_curve\n)\n\n# ----------------------- Utilities & Repro -----------------------\ndef set_seed(seed: int):\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed); torch.backends.cudnn.benchmark = True\n    print(f\"Seed set to {seed}.\")\n\n# ----------------------- Social-style Augmentations -----------------------\nclass RandomJPEGCompression:\n    def __init__(self, qmin=35, qmax=92, p=0.7): self.qmin, self.qmax, self.p = qmin, qmax, p\n    def __call__(self, img: Image.Image) -> Image.Image:\n        if random.random() > self.p: return img\n        buf = io.BytesIO(); img.save(buf, format=\"JPEG\", quality=random.randint(self.qmin, self.qmax))\n        buf.seek(0); return Image.open(buf).convert(\"RGB\")\nclass RandomDownscale:\n    def __init__(self, scale_min=0.4, scale_max=0.85, p=0.6): self.scale_min, self.scale_max, self.p = scale_min, scale_max, p\n    def __call__(self, img: Image.Image) -> Image.Image:\n        if random.random() > self.p: return img\n        w, h = img.size; s = random.uniform(self.scale_min, self.scale_max)\n        nw, nh = max(8, int(w*s)), max(8, int(h*s))\n        img = img.resize((nw, nh), resample=Image.BILINEAR); img = img.resize((w, h), resample=Image.BILINEAR)\n        return img\nclass RandomGaussianNoise:\n    def __init__(self, sigma_min=0.0, sigma_max=0.03, p=0.4): self.sigma_min, self.sigma_max, self.p = sigma_min, sigma_max, p\n    def __call__(self, img: Image.Image) -> Image.Image:\n        if random.random() > self.p: return img\n        arr = np.asarray(img).astype(np.float32)/255.0; sigma = random.uniform(self.sigma_min, self.sigma_max)\n        noise = np.random.normal(0.0, sigma, arr.shape).astype(np.float32); arr = np.clip(arr + noise, 0.0, 1.0)\n        arr = (arr*255.0 + 0.5).astype(np.uint8); return Image.fromarray(arr)\n\n# ----------------------- Custom Video Dataset -----------------------\nclass VideoFrameDataset(torch.utils.data.Dataset):\n    def __init__(self, root: Path, transform=None, class_to_idx=None):\n        self.root = root; self.transform = transform; self.class_to_idx = class_to_idx\n        self.samples = []\n        # ***** THE FIX IS HERE (1/2): Initialize targets list *****\n        self.targets = []\n        \n        for class_name in self.class_to_idx.keys():\n            class_dir = self.root / class_name\n            if not class_dir.exists(): continue\n            for video_path in sorted(list(class_dir.rglob('*.mp4'))):\n                label = self.class_to_idx[class_name]\n                self.samples.append((str(video_path), label))\n                # ***** THE FIX IS HERE (2/2): Add label to targets list *****\n                self.targets.append(label)\n\n        if not self.samples: print(f\"  --> WARNING: No .mp4 videos found in {self.root}\")\n        \n    def __len__(self): return len(self.samples)\n    def __getitem__(self, idx):\n        video_path, label = self.samples[idx]\n        cap = None\n        try:\n            cap = cv2.VideoCapture(video_path)\n            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n            if total_frames < 1: return self.__getitem__((idx + 1) % len(self))\n            frame_idx = random.randint(0, total_frames - 1)\n            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n            ret, frame = cap.read()\n            if not ret: return self.__getitem__((idx + 1) % len(self))\n        except Exception as e:\n            print(f\"  --> Error processing {video_path}: {e}. Skipping.\"); return self.__getitem__((idx + 1) % len(self))\n        finally:\n            if cap: cap.release()\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB); img = Image.fromarray(frame_rgb)\n        if self.transform: img = self.transform(img)\n        return img, label\n\n# ----------------------- Data Loading Pipeline -----------------------\ndef build_indices_limit_per_seq(dataset: ConcatDataset, max_per_seq=60, seed=42):\n    if max_per_seq is None or max_per_seq <= 0: return list(range(len(dataset)))\n    by_seq = defaultdict(list)\n    print(\"Building sequence-limited sampler for combined dataset...\")\n    for idx, (path, _) in enumerate(tqdm(dataset.samples, desc=\"Indexing sequences\")):\n        path_obj = Path(path)\n        seq_id = path_obj.stem if path_obj.suffix.lower() == '.mp4' else path_obj.parent.name\n        by_seq[seq_id].append(idx)\n    rng = random.Random(seed); indices = []\n    for seq, idxs in by_seq.items(): rng.shuffle(idxs); indices.extend(idxs[:max_per_seq])\n    rng.shuffle(indices); return indices\n\ndef build_loaders(image_data_root, video_data_root, img_size=224, batch_size=64, workers=2, per_seq_cap=60):\n    normalize = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n    train_tf = transforms.Compose([\n        transforms.Resize(int(img_size*1.2)), transforms.RandomResizedCrop(img_size, scale=(0.55, 1.0), ratio=(0.75, 1.33)),\n        transforms.RandomHorizontalFlip(p=0.5), transforms.RandomRotation(degrees=10),\n        transforms.RandomPerspective(distortion_scale=0.25, p=0.25), RandomJPEGCompression(qmin=35, qmax=92, p=0.7),\n        RandomDownscale(scale_min=0.4, scale_max=0.85, p=0.6), transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.5)),\n        transforms.ColorJitter(0.15, 0.15, 0.15, 0.05), transforms.RandomAdjustSharpness(sharpness_factor=0.6, p=0.3),\n        transforms.RandomAutocontrast(p=0.3), RandomGaussianNoise(sigma_min=0.0, sigma_max=0.03, p=0.4),\n        transforms.ToTensor(), normalize, transforms.RandomErasing(p=0.25, scale=(0.02,0.2), ratio=(0.3,3.3), value='random'),\n    ])\n    val_tf = transforms.Compose([transforms.Resize(int(img_size*1.15)), transforms.CenterCrop(img_size), transforms.ToTensor(), normalize])\n    train_datasets, val_datasets = [], []; master_class_to_idx = {'fake': 0, 'real': 1}\n    if image_data_root:\n        root = Path(image_data_root); print(f\"Loading IMAGE data from: {root}\")\n        if (root/\"train\").exists() and (root/\"val\").exists():\n            train_datasets.append(datasets.ImageFolder(root/\"train\", transform=train_tf))\n            val_datasets.append(datasets.ImageFolder(root/\"val\", transform=val_tf))\n        else: print(f\"  --> WARNING: 'train' or 'val' not found in {root}. Skipping.\")\n    if video_data_root:\n        root = Path(video_data_root); print(f\"Loading VIDEO data from: {root}\")\n        if (root/\"train\").exists() and (root/\"val\").exists():\n            train_datasets.append(VideoFrameDataset(root/\"train\", transform=train_tf, class_to_idx=master_class_to_idx))\n            val_datasets.append(VideoFrameDataset(root/\"val\", transform=val_tf, class_to_idx=master_class_to_idx))\n        else: print(f\"  --> WARNING: 'train' or 'val' not found in {root}. Skipping.\")\n    if not train_datasets: raise ValueError(\"No valid datasets were loaded. Check paths in USER CONFIGURATION.\")\n    train_ds = ConcatDataset(train_datasets); val_ds = ConcatDataset(val_datasets)\n    train_ds.class_to_idx = master_class_to_idx; train_ds.classes = list(master_class_to_idx.keys())\n    train_ds.samples = [s for ds in train_ds.datasets for s in ds.samples]\n    train_ds.targets = [t for ds in train_ds.datasets for t in ds.targets]\n    # Also create combined targets for the validation set for completeness\n    val_ds.targets = [t for ds in val_ds.datasets for t in ds.targets]\n\n    print(f\"\\nCombined {len(train_datasets)} dataset(s). Training samples: {len(train_ds)}, Validation samples: {len(val_ds)}\")\n    indices = build_indices_limit_per_seq(train_ds, max_per_seq=per_seq_cap, seed=42)\n    num_workers = 2 if os.cpu_count() <= 4 else 4\n    train_dl = DataLoader(train_ds, batch_size=batch_size, sampler=SubsetRandomSampler(indices), num_workers=num_workers, pin_memory=True, persistent_workers=(num_workers>0))\n    val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, persistent_workers=(num_workers>0))\n    return train_ds, val_ds, train_dl, val_dl\n\n# --- [The rest of the helper functions are unchanged] ---\nclass WarmupThenCosine(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self, optimizer, warmup_steps, total_steps, last_epoch=-1):\n        self.warmup_steps=max(1,warmup_steps); self.total_steps=max(self.warmup_steps+1,total_steps); super().__init__(optimizer, last_epoch)\n    def get_lr(self):\n        step = self.last_epoch + 1\n        if step <= self.warmup_steps: return [base * (step/self.warmup_steps) for base in self.base_lrs]\n        t = (step-self.warmup_steps)/(self.total_steps-self.warmup_steps); return [base*0.5*(1+math.cos(math.pi*t)) for base in self.base_lrs]\nclass EMA:\n    def __init__(self, model, decay=0.999): self.decay = decay; self.shadow = {k: v.detach().clone() for k, v in model.state_dict().items()}\n    @torch.no_grad()\n    def update(self, model):\n        for k, v in model.state_dict().items():\n            if not v.is_floating_point(): self.shadow[k] = v.detach().clone(); continue\n            if (self.shadow[k].dtype!=v.dtype)or(self.shadow[k].device!=v.device): self.shadow[k]=v.detach().clone(); continue\n            self.shadow[k].mul_(self.decay).add_(v.detach(), alpha=1.0 - self.decay)\ndef rand_bbox(W, H, lam):\n    cut_w=int(W*math.sqrt(1-lam)); cut_h=int(H*math.sqrt(1-lam)); cx,cy=random.randint(0,W-1),random.randint(0,H-1)\n    x1,y1=max(cx-cut_w//2,0),max(cy-cut_h//2,0); x2,y2=min(cx+cut_w//2,W),min(cy+cut_h//2,H); return x1,y1,x2,y2\ndef mixup_data(x, y, alpha=0.2):\n    lam=np.random.beta(alpha,alpha) if alpha>0 else 1.0; idx=torch.randperm(x.size(0),device=x.device)\n    mixed_x=lam*x+(1-lam)*x[idx]; y_a,y_b=y,y[idx]; return mixed_x,y_a,y_b,lam\ndef mixup_criterion(criterion, pred, y_a, y_b, lam): return lam*criterion(pred,y_a)+(1-lam)*criterion(pred,y_b)\n@torch.no_grad()\ndef tta_logits(model,x,device,tta=4):\n    outs=[]\n    for op in range(tta):\n        xx=x;\n        if op%2==1: xx=torch.flip(xx,dims=[-1])\n        with torch.amp.autocast(device_type=device.type, enabled=(device.type==\"cuda\")): outs.append(model(xx))\n    return torch.stack(outs,0).mean(0)\n@torch.no_grad()\ndef evaluate(model,dl,device,pos_index,tta=4):\n    model.eval(); y_true,y_pred,y_prob_pos=[],[],[]; total_loss,n=0.0,0\n    criterion=nn.CrossEntropyLoss()\n    for x,y in dl:\n        x,y=x.to(device,non_blocking=True),y.to(device,non_blocking=True)\n        logits=tta_logits(model,x,device,tta=tta) if tta and tta>1 else model(x)\n        with torch.amp.autocast(device_type=device.type,enabled=(device.type==\"cuda\")): loss=criterion(logits,y)\n        prob=torch.softmax(logits,dim=1)\n        y_true.extend(y.cpu().tolist());y_pred.extend(prob.argmax(1).cpu().tolist());y_prob_pos.extend(prob[:,pos_index].cpu().tolist())\n        bs=y.size(0);total_loss+=loss.item()*bs;n+=bs\n    acc=accuracy_score(y_true,y_pred); prec,rec,f1,_=precision_recall_fscore_support(y_true,y_pred,average=\"binary\",pos_label=pos_index,zero_division=0)\n    try: auc=roc_auc_score([1 if t==pos_index else 0 for t in y_true], y_prob_pos)\n    except Exception: auc=float(\"nan\")\n    cm=confusion_matrix(y_true,y_pred,labels=[0,1]); return (total_loss/n,acc,prec,rec,f1,auc,np.array(cm),np.array(y_true),np.array(y_prob_pos))\ndef plot_curves(history,out_dir:Path):\n    out_dir.mkdir(parents=True,exist_ok=True);epochs=np.arange(1,len(history[\"train_loss\"])+1)\n    plt.figure();plt.plot(epochs,history[\"train_loss\"],label=\"train loss\");plt.plot(epochs,history[\"val_loss\"],label=\"val loss\");plt.xlabel(\"epoch\");plt.ylabel(\"loss\");plt.legend();plt.tight_layout();plt.savefig(out_dir/\"loss_curves.png\");plt.close()\n    plt.figure();plt.plot(epochs,history[\"train_acc\"],label=\"train acc\");plt.plot(epochs,history[\"val_acc\"],label=\"val acc\");plt.xlabel(\"epoch\");plt.ylabel(\"accuracy\");plt.legend();plt.tight_layout();plt.savefig(out_dir/\"acc_curves.png\");plt.close()\ndef plot_cm(cm,out_dir:Path,class_names):\n    plt.figure();plt.imshow(cm,interpolation=\"nearest\");plt.title(\"Confusion Matrix\");plt.colorbar();ticks=np.arange(len(class_names));plt.xticks(ticks,class_names,rotation=45);plt.yticks(ticks,class_names);thresh=cm.max()/2.0\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]): plt.text(j,i,int(cm[i,j]),ha=\"center\",va=\"center\",color=\"white\" if cm[i,j]>thresh else \"black\")\n    plt.ylabel(\"True\");plt.xlabel(\"Predicted\");plt.tight_layout();plt.savefig(out_dir/\"confusion_matrix.png\");plt.close()\ndef plot_roc_and_save(y_true_bin,y_prob_pos,out_dir:Path,pos_label_name=\"fake\"):\n    fpr,tpr,_=roc_curve(y_true_bin,y_prob_pos);auc=roc_auc_score(y_true_bin,y_prob_pos)\n    plt.figure();plt.plot(fpr,tpr,label=f\"AUC={auc:.3f}\");plt.plot([0,1],[0,1],\"--\");plt.xlabel(\"FPR\");plt.ylabel(\"TPR\");plt.title(f\"ROC ({pos_label_name} positive)\");plt.legend();plt.tight_layout();plt.savefig(out_dir/\"roc_curve.png\");plt.close();return auc\n\n# ----------------------- Main -----------------------\ndef main():\n    \n    # =================================================================================\n    # ===> KAGGLE USER CONFIGURATION SECTION (CRITICAL!) <===\n    # THIS SECTION HAS BEEN PRE-FILLED BASED ON YOUR SCREENSHOT.\n    \n    # Path to the IMAGE dataset folder (the one containing train/ and val/).\n    USER_IMAGE_DATA_ROOT = \"/kaggle/input/faceforensics/WD_subset_png\"\n\n    # Path to the VIDEO dataset folder (the one containing train/ and val/).\n    USER_VIDEO_DATA_ROOT = \"/kaggle/input/wilddeepfake/ffpp_subset_c23\"\n\n    # Output is saved to the standard Kaggle working directory\n    USER_OUT_DIR = \"/kaggle/working/training_results\"\n    # =================================================================================\n\n    args_dict = {\n        \"image_data_root\": USER_IMAGE_DATA_ROOT, \"video_data_root\": USER_VIDEO_DATA_ROOT,\n        \"out_dir\": USER_OUT_DIR, \"per_seq_cap\": 60, \"epochs\": 40, \n        \"batch_size\": 128, \"img_size\": 224, \"workers\": 2,\n        \"lr\": 3e-4, \"weight_decay\": 1e-4, \"label_smoothing\": 0.05, \"freeze_backbone\": False,\n        \"class_weights\": True, \"mixup_p\": 0.3, \"cutmix_p\": 0.2, \"mix_alpha\": 0.2,\n        \"ema_decay\": 0.999, \"grad_clip\": 1.0, \"warmup_pct\": 0.1, \"tta\": 4, \"seed\": 42,\n    }\n    args = argparse.Namespace(**args_dict)\n\n    print(\"\\n[Kaggle Setup]\");\n    if not torch.cuda.is_available(): print(\"\\n!!!! WARNING: GPU ACCELERATOR NOT DETECTED !!!!\\n\")\n    else: print(f\"GPU Detected: {torch.cuda.get_device_name(0)}\")\n\n    set_seed(args.seed); device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); out_dir = Path(args.out_dir)\n    ckpt_dir = out_dir / \"checkpoints\"; art_dir = out_dir / \"artifacts\"\n    ckpt_dir.mkdir(parents=True, exist_ok=True); art_dir.mkdir(parents=True, exist_ok=True)\n    print(f\"\\nSaving outputs to: {out_dir}\")\n\n    train_ds, val_ds, train_dl, val_dl = build_loaders(\n        image_data_root=args.image_data_root, video_data_root=args.video_data_root,\n        img_size=args.img_size, batch_size=args.batch_size, workers=args.workers, per_seq_cap=args.per_seq_cap\n    )\n    print(f\"Master Classes: {train_ds.class_to_idx}\"); idx_fake = train_ds.class_to_idx.get(\"fake\", 0)\n\n    class_weights_tensor = None\n    if args.class_weights:\n        print(\"Computing class weights for combined dataset...\")\n        sampler_indices = list(train_dl.sampler.indices); labels = [train_ds.targets[i] for i in sampler_indices]\n        counts = np.bincount(labels, minlength=len(train_ds.classes)); total = counts.sum()\n        weights = [total/(len(counts)*c) if c > 0 else 0.0 for c in counts]\n        class_weights_tensor = torch.tensor(weights, dtype=torch.float32, device=device)\n        print(f\"Class counts in one epoch: {counts}. Weights: {class_weights_tensor.cpu().numpy()}\")\n\n    print(\"\\nInitializing ResNet-50 model...\"); model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n    model.fc = nn.Linear(model.fc.in_features, len(train_ds.classes))\n    if args.freeze_backbone: [p.requires_grad_(False) for n, p in model.named_parameters() if not n.startswith(\"fc.\")]\n    model.to(device)\n    \n    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=args.label_smoothing)\n    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, weight_decay=args.weight_decay)\n    total_steps = len(train_dl) * args.epochs; warmup_steps = int(args.warmup_pct * total_steps)\n    scheduler = WarmupThenCosine(optimizer, warmup_steps=warmup_steps, total_steps=total_steps)\n    scaler = torch.amp.GradScaler(enabled=(device.type==\"cuda\"))\n    ema = EMA(model, decay=args.ema_decay)\n    history = defaultdict(list); best_auc, best_epoch = -1.0, -1; global_step = 0\n    \n    print(\"\\nStarting Training...\")\n    for epoch in range(1, args.epochs+1):\n        model.train(); tr_loss_sum, tr_correct, n_samples = 0.0, 0, 0\n        pbar = tqdm(train_dl, desc=f\"Epoch {epoch}/{args.epochs}\", leave=False)\n        for x, y in pbar:\n            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            use_mixup = random.random() < args.mixup_p; use_cutmix = (not use_mixup) and (random.random() < args.cutmix_p)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.amp.autocast(device_type=device.type, enabled=(device.type==\"cuda\")):\n                if use_mixup:\n                    x_mix, y_a, y_b, lam = mixup_data(x, y, alpha=args.mix_alpha)\n                    logits = model(x_mix); loss = mixup_criterion(criterion, logits, y_a, y_b, lam); y_for_acc = y_a\n                elif use_cutmix:\n                    lam = np.random.beta(args.mix_alpha, args.mix_alpha); idx_perm = torch.randperm(x.size(0), device=x.device)\n                    y_a, y_b = y, y[idx_perm]; W, H = x.shape[3], x.shape[2]; x1, y1, x2, y2 = rand_bbox(W, H, lam)\n                    x[:, :, y1:y2, x1:x2] = x[idx_perm, :, y1:y2, x1:x2]; lam_adj = 1 - ((x2 - x1) * (y2 - y1) / (W * H))\n                    logits = model(x); loss = mixup_criterion(criterion, logits, y_a, y_b, lam_adj); y_for_acc = y_a\n                else: logits = model(x); loss = criterion(logits, y); y_for_acc = y\n            scaler.scale(loss).backward(); scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n            scaler.step(optimizer); scaler.update(); ema.update(model); scheduler.step(); global_step += 1\n            bs = y.size(0); tr_loss_sum += loss.item() * bs; tr_correct += (logits.detach().argmax(1) == y_for_acc).sum().item(); n_samples += bs\n            pbar.set_postfix(loss=tr_loss_sum/max(1,n_samples), acc=tr_correct/max(1,n_samples))\n        tr_loss = tr_loss_sum / max(1, n_samples); tr_acc  = tr_correct / max(1, n_samples)\n        state_backup = {k: v.detach().clone() for k, v in model.state_dict().items()}\n        model.load_state_dict(ema.shadow, strict=True)\n        val_loss, val_acc, prec, rec, f1, val_auc, cm, y_true, y_prob_pos = evaluate(model, val_dl, device, pos_index=idx_fake, tta=args.tta)\n        model.load_state_dict(state_backup, strict=True)\n        ckpt = {\"model\": ema.shadow, \"epoch\": epoch, \"val_auc\": float(val_auc), \"class_to_idx\": train_ds.class_to_idx, \"args\": args_dict}\n        torch.save(ckpt, ckpt_dir / f\"resnet50_epoch{epoch:02d}.pt\")\n        history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc); history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc); history[\"val_auc\"].append(val_auc); history[\"val_f1\"].append(f1)\n        tqdm.write(f\"Epoch {epoch:02d} | tr Ls {tr_loss:.4f} Ac {tr_acc:.4f} | val Ls {val_loss:.4f} Ac {val_acc:.4f} AUC {val_auc:.4f} F1 {f1:.4f}\")\n        class_names = [k for k,_ in sorted(train_ds.class_to_idx.items(), key=lambda kv: kv[1])]\n        plot_cm(cm, art_dir, class_names=class_names)\n        y_true_bin = (y_true == idx_fake).astype(int)\n        _ = plot_roc_and_save(y_true_bin, y_prob_pos, art_dir, pos_label_name=\"fake\")\n        if not math.isnan(val_auc) and val_auc > best_auc:\n            best_auc, best_epoch = val_auc, epoch; torch.save(ckpt, ckpt_dir/\"resnet50_best.pt\")\n            tqdm.write(f\"  >>> New Best AUC: {best_auc:.4f} saved.\")\n    plot_curves(history, art_dir);\n    with open(out_dir/\"history.json\",\"w\") as f: json.dump(history, f, indent=2)\n    print(f\"\\nTraining Complete.\\nBest epoch by AUC: {best_epoch} (AUC={best_auc:.4f})\\nCheckpoints: {ckpt_dir}\\nArtifacts: {art_dir}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T06:53:12.228519Z","iopub.execute_input":"2025-10-11T06:53:12.229234Z","iopub.status.idle":"2025-10-11T08:06:32.436846Z","shell.execute_reply.started":"2025-10-11T06:53:12.229206Z","shell.execute_reply":"2025-10-11T08:06:32.436102Z"}},"outputs":[{"name":"stdout","text":"\n[Kaggle Setup]\nGPU Detected: Tesla T4\nSeed set to 42.\n\nSaving outputs to: /kaggle/working/training_results\nLoading IMAGE data from: /kaggle/input/faceforensics/WD_subset_png\nLoading VIDEO data from: /kaggle/input/wilddeepfake/ffpp_subset_c23\n\nCombined 2 dataset(s). Training samples: 47927, Validation samples: 5219\nBuilding sequence-limited sampler for combined dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Indexing sequences:   0%|          | 0/47927 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb4bc59c263d41f48866b2adfd5bab13"}},"metadata":{}},{"name":"stdout","text":"Master Classes: {'fake': 0, 'real': 1}\nComputing class weights for combined dataset...\nClass counts in one epoch: [1211 1530]. Weights: [1.1317093  0.89575166]\n\nInitializing ResNet-50 model...\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 160MB/s] \n","output_type":"stream"},{"name":"stdout","text":"\nStarting Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 01 | tr Ls 0.6798 Ac 0.5662 | val Ls 0.6698 Ac 0.5576 AUC 0.6793 F1 0.0625\n  >>> New Best AUC: 0.6793 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 02 | tr Ls 0.5364 Ac 0.7125 | val Ls 0.6688 Ac 0.5612 AUC 0.6889 F1 0.0803\n  >>> New Best AUC: 0.6889 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 03 | tr Ls 0.3972 Ac 0.7935 | val Ls 0.6673 Ac 0.5620 AUC 0.7027 F1 0.0856\n  >>> New Best AUC: 0.7027 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 04 | tr Ls 0.3373 Ac 0.8927 | val Ls 0.6664 Ac 0.5664 AUC 0.7099 F1 0.1087\n  >>> New Best AUC: 0.7099 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 05 | tr Ls 0.2574 Ac 0.8595 | val Ls 0.6660 Ac 0.5677 AUC 0.7139 F1 0.1330\n  >>> New Best AUC: 0.7139 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 06 | tr Ls 0.2697 Ac 0.8997 | val Ls 0.6657 Ac 0.5731 AUC 0.7155 F1 0.1637\n  >>> New Best AUC: 0.7155 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 07 | tr Ls 0.2505 Ac 0.8781 | val Ls 0.6652 Ac 0.5794 AUC 0.7185 F1 0.1992\n  >>> New Best AUC: 0.7185 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 08 | tr Ls 0.2476 Ac 0.8763 | val Ls 0.6650 Ac 0.5850 AUC 0.7175 F1 0.2405\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 09 | tr Ls 0.2380 Ac 0.9230 | val Ls 0.6652 Ac 0.5917 AUC 0.7114 F1 0.2913\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10 | tr Ls 0.2005 Ac 0.9307 | val Ls 0.6659 Ac 0.6049 AUC 0.7043 F1 0.3612\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 11/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 11 | tr Ls 0.2089 Ac 0.9489 | val Ls 0.6667 Ac 0.6095 AUC 0.6968 F1 0.4323\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 12/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 12 | tr Ls 0.2399 Ac 0.8760 | val Ls 0.6674 Ac 0.6197 AUC 0.6923 F1 0.5066\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 13/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 13 | tr Ls 0.2834 Ac 0.8701 | val Ls 0.6675 Ac 0.6386 AUC 0.6942 F1 0.5718\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 14/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 14 | tr Ls 0.2382 Ac 0.9044 | val Ls 0.6678 Ac 0.6497 AUC 0.6965 F1 0.6193\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 15/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 15 | tr Ls 0.1743 Ac 0.9289 | val Ls 0.6669 Ac 0.6507 AUC 0.7036 F1 0.6416\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 16/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 16 | tr Ls 0.2038 Ac 0.9354 | val Ls 0.6660 Ac 0.6526 AUC 0.7093 F1 0.6634\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 17/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 17 | tr Ls 0.2456 Ac 0.9110 | val Ls 0.6641 Ac 0.6480 AUC 0.7194 F1 0.6723\n  >>> New Best AUC: 0.7194 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 18/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 18 | tr Ls 0.2201 Ac 0.8767 | val Ls 0.6625 Ac 0.6371 AUC 0.7284 F1 0.6737\n  >>> New Best AUC: 0.7284 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 19/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 19 | tr Ls 0.2392 Ac 0.8570 | val Ls 0.6600 Ac 0.6306 AUC 0.7411 F1 0.6756\n  >>> New Best AUC: 0.7411 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 20/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 20 | tr Ls 0.2423 Ac 0.8763 | val Ls 0.6576 Ac 0.6177 AUC 0.7512 F1 0.6718\n  >>> New Best AUC: 0.7512 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 21/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 21 | tr Ls 0.2199 Ac 0.8738 | val Ls 0.6547 Ac 0.6135 AUC 0.7625 F1 0.6721\n  >>> New Best AUC: 0.7625 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 22/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 22 | tr Ls 0.1956 Ac 0.9296 | val Ls 0.6511 Ac 0.6093 AUC 0.7750 F1 0.6715\n  >>> New Best AUC: 0.7750 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 23/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 23 | tr Ls 0.2014 Ac 0.8858 | val Ls 0.6475 Ac 0.6097 AUC 0.7858 F1 0.6734\n  >>> New Best AUC: 0.7858 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 24/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeca7cd88db24a6593ac26d578bb3204"}},"metadata":{}},{"name":"stdout","text":"Epoch 24 | tr Ls 0.2118 Ac 0.8599 | val Ls 0.6433 Ac 0.6103 AUC 0.7970 F1 0.6751\n  >>> New Best AUC: 0.7970 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 25/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cfc56db339b476dac96fc056c0d57f5"}},"metadata":{}},{"name":"stdout","text":"Epoch 25 | tr Ls 0.2216 Ac 0.8986 | val Ls 0.6391 Ac 0.6122 AUC 0.8078 F1 0.6768\n  >>> New Best AUC: 0.8078 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 26/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 26 | tr Ls 0.1984 Ac 0.9460 | val Ls 0.6341 Ac 0.6168 AUC 0.8189 F1 0.6805\n  >>> New Best AUC: 0.8189 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 27/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 27 | tr Ls 0.1721 Ac 0.9165 | val Ls 0.6287 Ac 0.6218 AUC 0.8279 F1 0.6835\n  >>> New Best AUC: 0.8279 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 28/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 28 | tr Ls 0.2337 Ac 0.8614 | val Ls 0.6224 Ac 0.6269 AUC 0.8376 F1 0.6864\n  >>> New Best AUC: 0.8376 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 29/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 29 | tr Ls 0.2171 Ac 0.9000 | val Ls 0.6161 Ac 0.6317 AUC 0.8454 F1 0.6895\n  >>> New Best AUC: 0.8454 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 30/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 30 | tr Ls 0.2106 Ac 0.8435 | val Ls 0.6100 Ac 0.6390 AUC 0.8516 F1 0.6939\n  >>> New Best AUC: 0.8516 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 31/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 31 | tr Ls 0.2085 Ac 0.8825 | val Ls 0.6037 Ac 0.6465 AUC 0.8574 F1 0.6983\n  >>> New Best AUC: 0.8574 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 32/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 32 | tr Ls 0.2158 Ac 0.8325 | val Ls 0.5976 Ac 0.6526 AUC 0.8622 F1 0.7020\n  >>> New Best AUC: 0.8622 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 33/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 33 | tr Ls 0.1963 Ac 0.9267 | val Ls 0.5912 Ac 0.6599 AUC 0.8669 F1 0.7063\n  >>> New Best AUC: 0.8669 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 34/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 34 | tr Ls 0.1724 Ac 0.8172 | val Ls 0.5842 Ac 0.6660 AUC 0.8718 F1 0.7099\n  >>> New Best AUC: 0.8718 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 35/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 35 | tr Ls 0.2050 Ac 0.8709 | val Ls 0.5773 Ac 0.6722 AUC 0.8757 F1 0.7137\n  >>> New Best AUC: 0.8757 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 36/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 36 | tr Ls 0.2139 Ac 0.8858 | val Ls 0.5707 Ac 0.6775 AUC 0.8792 F1 0.7172\n  >>> New Best AUC: 0.8792 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 37/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 37 | tr Ls 0.2628 Ac 0.8971 | val Ls 0.5636 Ac 0.6815 AUC 0.8829 F1 0.7195\n  >>> New Best AUC: 0.8829 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 38/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 38 | tr Ls 0.1802 Ac 0.8690 | val Ls 0.5575 Ac 0.6858 AUC 0.8854 F1 0.7222\n  >>> New Best AUC: 0.8854 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 39/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 39 | tr Ls 0.2388 Ac 0.8530 | val Ls 0.5508 Ac 0.6911 AUC 0.8881 F1 0.7256\n  >>> New Best AUC: 0.8881 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 40/40:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 40 | tr Ls 0.2053 Ac 0.8854 | val Ls 0.5440 Ac 0.6973 AUC 0.8908 F1 0.7294\n  >>> New Best AUC: 0.8908 saved.\n\nTraining Complete.\nBest epoch by AUC: 40 (AUC=0.8908)\nCheckpoints: /kaggle/working/training_results/checkpoints\nArtifacts: /kaggle/working/training_results/artifacts\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\"\"\"\nResNet-50 Deepfake Detector - Kaggle GPU Optimized\nVERSION FOR COMBINED IMAGE & VIDEO DATASETS WITH RESUME CAPABILITY\n\"\"\"\n\nimport argparse, io, json, math, os, random, time, sys\nfrom pathlib import Path\nfrom collections import defaultdict\n\n# --- [SETUP] Install necessary libraries ---\ntry:\n    import cv2\nexcept ImportError:\n    print(\"OpenCV not found. Installing opencv-python-headless...\")\n    import subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"opencv-python-headless\"])\n    import cv2\n\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, SubsetRandomSampler, ConcatDataset\nfrom torchvision import datasets, transforms, models\n\nfrom sklearn.metrics import (\n    accuracy_score, precision_recall_fscore_support,\n    confusion_matrix, roc_auc_score, roc_curve\n)\n\n# ----------------------- Utilities & Repro -----------------------\ndef set_seed(seed: int):\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed); torch.backends.cudnn.benchmark = True\n    print(f\"Seed set to {seed}.\")\n\n# ----------------------- Social-style Augmentations -----------------------\nclass RandomJPEGCompression:\n    def __init__(self, qmin=35, qmax=92, p=0.7): self.qmin, self.qmax, self.p = qmin, qmax, p\n    def __call__(self, img: Image.Image) -> Image.Image:\n        if random.random() > self.p: return img\n        buf = io.BytesIO(); img.save(buf, format=\"JPEG\", quality=random.randint(self.qmin, self.qmax))\n        buf.seek(0); return Image.open(buf).convert(\"RGB\")\nclass RandomDownscale:\n    def __init__(self, scale_min=0.4, scale_max=0.85, p=0.6): self.scale_min, self.scale_max, self.p = scale_min, scale_max, p\n    def __call__(self, img: Image.Image) -> Image.Image:\n        if random.random() > self.p: return img\n        w, h = img.size; s = random.uniform(self.scale_min, self.scale_max)\n        nw, nh = max(8, int(w*s)), max(8, int(h*s))\n        img = img.resize((nw, nh), resample=Image.BILINEAR); img = img.resize((w, h), resample=Image.BILINEAR)\n        return img\nclass RandomGaussianNoise:\n    def __init__(self, sigma_min=0.0, sigma_max=0.03, p=0.4): self.sigma_min, self.sigma_max, self.p = sigma_min, sigma_max, p\n    def __call__(self, img: Image.Image) -> Image.Image:\n        if random.random() > self.p: return img\n        arr = np.asarray(img).astype(np.float32)/255.0; sigma = random.uniform(self.sigma_min, self.sigma_max)\n        noise = np.random.normal(0.0, sigma, arr.shape).astype(np.float32); arr = np.clip(arr + noise, 0.0, 1.0)\n        arr = (arr*255.0 + 0.5).astype(np.uint8); return Image.fromarray(arr)\n\n# ----------------------- Custom Video Dataset -----------------------\nclass VideoFrameDataset(torch.utils.data.Dataset):\n    def __init__(self, root: Path, transform=None, class_to_idx=None):\n        self.root = root; self.transform = transform; self.class_to_idx = class_to_idx\n        self.samples = []; self.targets = []\n        for class_name in self.class_to_idx.keys():\n            class_dir = self.root / class_name\n            if not class_dir.exists(): continue\n            for video_path in sorted(list(class_dir.rglob('*.mp4'))):\n                label = self.class_to_idx[class_name]\n                self.samples.append((str(video_path), label)); self.targets.append(label)\n        if not self.samples: print(f\"  --> WARNING: No .mp4 videos found in {self.root}\")\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, idx):\n        video_path, label = self.samples[idx]\n        cap = None\n        try:\n            cap = cv2.VideoCapture(video_path)\n            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n            if total_frames < 1: return self.__getitem__((idx + 1) % len(self))\n            frame_idx = random.randint(0, total_frames - 1)\n            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n            ret, frame = cap.read()\n            if not ret: return self.__getitem__((idx + 1) % len(self))\n        except Exception as e:\n            print(f\"  --> Error processing {video_path}: {e}. Skipping.\"); return self.__getitem__((idx + 1) % len(self))\n        finally:\n            if cap: cap.release()\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB); img = Image.fromarray(frame_rgb)\n        if self.transform: img = self.transform(img)\n        return img, label\n\n# ----------------------- Data Loading Pipeline -----------------------\ndef build_indices_limit_per_seq(dataset: ConcatDataset, max_per_seq=60, seed=42):\n    if max_per_seq is None or max_per_seq <= 0: return list(range(len(dataset)))\n    by_seq = defaultdict(list)\n    print(\"Building sequence-limited sampler for combined dataset...\")\n    for idx, (path, _) in enumerate(tqdm(dataset.samples, desc=\"Indexing sequences\")):\n        path_obj = Path(path)\n        seq_id = path_obj.stem if path_obj.suffix.lower() == '.mp4' else path_obj.parent.name\n        by_seq[seq_id].append(idx)\n    rng = random.Random(seed); indices = []\n    for seq, idxs in by_seq.items(): rng.shuffle(idxs); indices.extend(idxs[:max_per_seq])\n    rng.shuffle(indices); return indices\n\ndef build_loaders(image_data_root, video_data_root, img_size=224, batch_size=64, workers=2, per_seq_cap=60):\n    normalize = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n    train_tf = transforms.Compose([\n        transforms.Resize(int(img_size*1.2)), transforms.RandomResizedCrop(img_size, scale=(0.55, 1.0), ratio=(0.75, 1.33)),\n        transforms.RandomHorizontalFlip(p=0.5), transforms.RandomRotation(degrees=10),\n        transforms.RandomPerspective(distortion_scale=0.25, p=0.25), RandomJPEGCompression(qmin=35, qmax=92, p=0.7),\n        RandomDownscale(scale_min=0.4, scale_max=0.85, p=0.6), transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.5)),\n        transforms.ColorJitter(0.15, 0.15, 0.15, 0.05), transforms.RandomAdjustSharpness(sharpness_factor=0.6, p=0.3),\n        transforms.RandomAutocontrast(p=0.3), RandomGaussianNoise(sigma_min=0.0, sigma_max=0.03, p=0.4),\n        transforms.ToTensor(), normalize, transforms.RandomErasing(p=0.25, scale=(0.02,0.2), ratio=(0.3,3.3), value='random'),\n    ])\n    val_tf = transforms.Compose([transforms.Resize(int(img_size*1.15)), transforms.CenterCrop(img_size), transforms.ToTensor(), normalize])\n    train_datasets, val_datasets = [], []; master_class_to_idx = {'fake': 0, 'real': 1}\n    if image_data_root:\n        root = Path(image_data_root); print(f\"Loading IMAGE data from: {root}\")\n        if (root/\"train\").exists() and (root/\"val\").exists():\n            train_datasets.append(datasets.ImageFolder(root/\"train\", transform=train_tf))\n            val_datasets.append(datasets.ImageFolder(root/\"val\", transform=val_tf))\n        else: print(f\"  --> WARNING: 'train' or 'val' not found in {root}. Skipping.\")\n    if video_data_root:\n        root = Path(video_data_root); print(f\"Loading VIDEO data from: {root}\")\n        if (root/\"train\").exists() and (root/\"val\").exists():\n            train_datasets.append(VideoFrameDataset(root/\"train\", transform=train_tf, class_to_idx=master_class_to_idx))\n            val_datasets.append(VideoFrameDataset(root/\"val\", transform=val_tf, class_to_idx=master_class_to_idx))\n        else: print(f\"  --> WARNING: 'train' or 'val' not found in {root}. Skipping.\")\n    if not train_datasets: raise ValueError(\"No valid datasets were loaded. Check paths in USER CONFIGURATION.\")\n    train_ds = ConcatDataset(train_datasets); val_ds = ConcatDataset(val_datasets)\n    train_ds.class_to_idx = master_class_to_idx; train_ds.classes = list(master_class_to_idx.keys())\n    train_ds.samples = [s for ds in train_ds.datasets for s in ds.samples]\n    train_ds.targets = [t for ds in train_ds.datasets for t in ds.targets]\n    val_ds.targets = [t for ds in val_ds.datasets for t in ds.targets]\n    print(f\"\\nCombined {len(train_datasets)} dataset(s). Training samples: {len(train_ds)}, Validation samples: {len(val_ds)}\")\n    indices = build_indices_limit_per_seq(train_ds, max_per_seq=per_seq_cap, seed=42)\n    num_workers = 2 if os.cpu_count() <= 4 else 4\n    train_dl = DataLoader(train_ds, batch_size=batch_size, sampler=SubsetRandomSampler(indices), num_workers=num_workers, pin_memory=True, persistent_workers=(num_workers>0))\n    val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, persistent_workers=(num_workers>0))\n    return train_ds, val_ds, train_dl, val_dl\n\n# --- [The rest of the helper functions are unchanged] ---\nclass WarmupThenCosine(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self, optimizer, warmup_steps, total_steps, last_epoch=-1): self.warmup_steps=max(1,warmup_steps); self.total_steps=max(self.warmup_steps+1,total_steps); super().__init__(optimizer, last_epoch)\n    def get_lr(self):\n        step = self.last_epoch + 1\n        if step <= self.warmup_steps: return [base * (step/self.warmup_steps) for base in self.base_lrs]\n        t = (step-self.warmup_steps)/(self.total_steps-self.warmup_steps); return [base*0.5*(1+math.cos(math.pi*t)) for base in self.base_lrs]\nclass EMA:\n    def __init__(self, model, decay=0.999): self.decay = decay; self.shadow = {k: v.detach().clone() for k, v in model.state_dict().items()}\n    @torch.no_grad()\n    def update(self, model):\n        for k, v in model.state_dict().items():\n            if not v.is_floating_point(): self.shadow[k] = v.detach().clone(); continue\n            if (self.shadow[k].dtype!=v.dtype)or(self.shadow[k].device!=v.device): self.shadow[k]=v.detach().clone(); continue\n            self.shadow[k].mul_(self.decay).add_(v.detach(), alpha=1.0 - self.decay)\ndef rand_bbox(W, H, lam):\n    cut_w=int(W*math.sqrt(1-lam)); cut_h=int(H*math.sqrt(1-lam)); cx,cy=random.randint(0,W-1),random.randint(0,H-1)\n    x1,y1=max(cx-cut_w//2,0),max(cy-cut_h//2,0); x2,y2=min(cx+cut_w//2,W),min(cy+cut_h//2,H); return x1,y1,x2,y2\ndef mixup_data(x, y, alpha=0.2):\n    lam=np.random.beta(alpha,alpha) if alpha>0 else 1.0; idx=torch.randperm(x.size(0),device=x.device)\n    mixed_x=lam*x+(1-lam)*x[idx]; y_a,y_b=y,y[idx]; return mixed_x,y_a,y_b,lam\ndef mixup_criterion(criterion, pred, y_a, y_b, lam): return lam*criterion(pred,y_a)+(1-lam)*criterion(pred,y_b)\n@torch.no_grad()\ndef tta_logits(model,x,device,tta=4):\n    outs=[]\n    for op in range(tta):\n        xx=x;\n        if op%2==1: xx=torch.flip(xx,dims=[-1])\n        with torch.amp.autocast(device_type=device.type, enabled=(device.type==\"cuda\")): outs.append(model(xx))\n    return torch.stack(outs,0).mean(0)\n@torch.no_grad()\ndef evaluate(model,dl,device,pos_index,tta=4):\n    model.eval(); y_true,y_pred,y_prob_pos=[],[],[]; total_loss,n=0.0,0\n    criterion=nn.CrossEntropyLoss()\n    for x,y in dl:\n        x,y=x.to(device,non_blocking=True),y.to(device,non_blocking=True)\n        logits=tta_logits(model,x,device,tta=tta) if tta and tta>1 else model(x)\n        with torch.amp.autocast(device_type=device.type,enabled=(device.type==\"cuda\")): loss=criterion(logits,y)\n        prob=torch.softmax(logits,dim=1)\n        y_true.extend(y.cpu().tolist());y_pred.extend(prob.argmax(1).cpu().tolist());y_prob_pos.extend(prob[:,pos_index].cpu().tolist())\n        bs=y.size(0);total_loss+=loss.item()*bs;n+=bs\n    acc=accuracy_score(y_true,y_pred); prec,rec,f1,_=precision_recall_fscore_support(y_true,y_pred,average=\"binary\",pos_label=pos_index,zero_division=0)\n    try: auc=roc_auc_score([1 if t==pos_index else 0 for t in y_true], y_prob_pos)\n    except Exception: auc=float(\"nan\")\n    cm=confusion_matrix(y_true,y_pred,labels=[0,1]); return (total_loss/n,acc,prec,rec,f1,auc,np.array(cm),np.array(y_true),np.array(y_prob_pos))\ndef plot_curves(history,out_dir:Path):\n    out_dir.mkdir(parents=True,exist_ok=True);epochs=np.arange(1,len(history[\"train_loss\"])+1)\n    plt.figure();plt.plot(epochs,history[\"train_loss\"],label=\"train loss\");plt.plot(epochs,history[\"val_loss\"],label=\"val loss\");plt.xlabel(\"epoch\");plt.ylabel(\"loss\");plt.legend();plt.tight_layout();plt.savefig(out_dir/\"loss_curves.png\");plt.close()\n    plt.figure();plt.plot(epochs,history[\"train_acc\"],label=\"train acc\");plt.plot(epochs,history[\"val_acc\"],label=\"val acc\");plt.xlabel(\"epoch\");plt.ylabel(\"accuracy\");plt.legend();plt.tight_layout();plt.savefig(out_dir/\"acc_curves.png\");plt.close()\ndef plot_cm(cm,out_dir:Path,class_names):\n    plt.figure();plt.imshow(cm,interpolation=\"nearest\");plt.title(\"Confusion Matrix\");plt.colorbar();ticks=np.arange(len(class_names));plt.xticks(ticks,class_names,rotation=45);plt.yticks(ticks,class_names);thresh=cm.max()/2.0\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]): plt.text(j,i,int(cm[i,j]),ha=\"center\",va=\"center\",color=\"white\" if cm[i,j]>thresh else \"black\")\n    plt.ylabel(\"True\");plt.xlabel(\"Predicted\");plt.tight_layout();plt.savefig(out_dir/\"confusion_matrix.png\");plt.close()\ndef plot_roc_and_save(y_true_bin,y_prob_pos,out_dir:Path,pos_label_name=\"fake\"):\n    fpr,tpr,_=roc_curve(y_true_bin,y_prob_pos);auc=roc_auc_score(y_true_bin,y_prob_pos)\n    plt.figure();plt.plot(fpr,tpr,label=f\"AUC={auc:.3f}\");plt.plot([0,1],[0,1],\"--\");plt.xlabel(\"FPR\");plt.ylabel(\"TPR\");plt.title(f\"ROC ({pos_label_name} positive)\");plt.legend();plt.tight_layout();plt.savefig(out_dir/\"roc_curve.png\");plt.close();return auc\n\n# ----------------------- Main -----------------------\ndef main():\n    \n    # =================================================================================\n    # ===> KAGGLE USER CONFIGURATION SECTION (CRITICAL!) <===\n    USER_IMAGE_DATA_ROOT = \"/kaggle/input/faceforensics/WD_subset_png\"\n    USER_VIDEO_DATA_ROOT = \"/kaggle/input/wilddeepfake/ffpp_subset_c23\"\n    USER_OUT_DIR = \"/kaggle/working/training_results\"\n\n    # ===> NEW: RESUME TRAINING CONFIGURATION <===\n    RESUME_TRAINING = True # SET THIS TO True TO RESUME\n    # This path points to the 'latest' checkpoint inside your output directory\n    CHECKPOINT_PATH = f\"{USER_OUT_DIR}/checkpoints/resnet50_epoch40.pt\"\n    # =================================================================================\n\n    args_dict = {\n        \"image_data_root\": USER_IMAGE_DATA_ROOT, \"video_data_root\": USER_VIDEO_DATA_ROOT,\n        \"out_dir\": USER_OUT_DIR, \"per_seq_cap\": 60, \n        \"epochs\": 60, # <== INCREASE THE TOTAL NUMBER OF EPOCHS\n        \"batch_size\": 128, \"img_size\": 224, \"workers\": 2,\n        \"lr\": 3e-4, \"weight_decay\": 1e-4, \"label_smoothing\": 0.05, \"freeze_backbone\": False,\n        \"class_weights\": True, \"mixup_p\": 0.3, \"cutmix_p\": 0.2, \"mix_alpha\": 0.2,\n        \"ema_decay\": 0.999, \"grad_clip\": 1.0, \"warmup_pct\": 0.1, \"tta\": 4, \"seed\": 42,\n    }\n    args = argparse.Namespace(**args_dict)\n\n    print(\"\\n[Kaggle Setup]\");\n    if not torch.cuda.is_available(): print(\"\\n!!!! WARNING: GPU ACCELERATOR NOT DETECTED !!!!\\n\")\n    else: print(f\"GPU Detected: {torch.cuda.get_device_name(0)}\")\n\n    set_seed(args.seed); device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); out_dir = Path(args.out_dir)\n    ckpt_dir = out_dir / \"checkpoints\"; art_dir = out_dir / \"artifacts\"\n    ckpt_dir.mkdir(parents=True, exist_ok=True); art_dir.mkdir(parents=True, exist_ok=True)\n    print(f\"\\nSaving outputs to: {out_dir}\")\n\n    train_ds, val_ds, train_dl, val_dl = build_loaders(\n        image_data_root=args.image_data_root, video_data_root=args.video_data_root,\n        img_size=args.img_size, batch_size=args.batch_size, workers=args.workers, per_seq_cap=args.per_seq_cap\n    )\n    print(f\"Master Classes: {train_ds.class_to_idx}\"); idx_fake = train_ds.class_to_idx.get(\"fake\", 0)\n\n    class_weights_tensor = None\n    if args.class_weights:\n        print(\"Computing class weights for combined dataset...\")\n        sampler_indices = list(train_dl.sampler.indices); labels = [train_ds.targets[i] for i in sampler_indices]\n        counts = np.bincount(labels, minlength=len(train_ds.classes)); total = counts.sum()\n        weights = [total/(len(counts)*c) if c > 0 else 0.0 for c in counts]\n        class_weights_tensor = torch.tensor(weights, dtype=torch.float32, device=device)\n        print(f\"Class counts in one epoch: {counts}. Weights: {class_weights_tensor.cpu().numpy()}\")\n\n    print(\"\\nInitializing ResNet-50 model...\"); model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n    model.fc = nn.Linear(model.fc.in_features, len(train_ds.classes))\n    if args.freeze_backbone: [p.requires_grad_(False) for n, p in model.named_parameters() if not n.startswith(\"fc.\")]\n    model.to(device)\n    \n    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=args.label_smoothing)\n    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, weight_decay=args.weight_decay)\n    total_steps = len(train_dl) * args.epochs; warmup_steps = int(args.warmup_pct * total_steps)\n    scheduler = WarmupThenCosine(optimizer, warmup_steps=warmup_steps, total_steps=total_steps)\n    scaler = torch.amp.GradScaler(enabled=(device.type==\"cuda\"))\n    ema = EMA(model, decay=args.ema_decay)\n    history = defaultdict(list); best_auc, best_epoch = -1.0, -1; global_step = 0; start_epoch = 1\n\n    # ===> NEW: LOGIC TO LOAD CHECKPOINT AND RESUME <===\n    if RESUME_TRAINING:\n        print(\"\\n[Resuming Training]\")\n        if not Path(CHECKPOINT_PATH).exists():\n            print(f\"  !!! WARNING: Checkpoint not found at {CHECKPOINT_PATH}. Starting from scratch. !!!\")\n        else:\n            # For modern PyTorch, weights_only=False is needed for non-tensor data\n            ckpt = torch.load(CHECKPOINT_PATH, map_location='cpu', weights_only=False)\n            \n            # The saved model is the EMA model\n            model.load_state_dict(ckpt['model'], strict=True) \n            model.to(device)\n            ema.shadow = {k: v.to(device) for k, v in ckpt['model'].items()}\n            \n            start_epoch = ckpt.get('epoch', 0) + 1\n            \n            print(f\"  Fast-forwarding LR scheduler to step of epoch {start_epoch - 1}...\")\n            steps_to_advance = (start_epoch - 1) * len(train_dl)\n            for _ in range(steps_to_advance): scheduler.step()\n\n            history_path = out_dir / \"history.json\"\n            if history_path.exists():\n                with open(history_path, 'r') as f: history = defaultdict(list, json.load(f))\n                if history.get('val_auc'): best_auc = max(history.get('val_auc', [0.0]))\n            print(f\"  --> Checkpoint loaded. Resuming from Epoch {start_epoch}. Previous best AUC: {best_auc:.4f}\")\n\n    print(\"\\nStarting Training...\")\n    # The loop now starts from 'start_epoch'\n    for epoch in range(start_epoch, args.epochs + 1):\n        model.train(); tr_loss_sum, tr_correct, n_samples = 0.0, 0, 0\n        pbar = tqdm(train_dl, desc=f\"Epoch {epoch}/{args.epochs}\", leave=False)\n        for x, y in pbar:\n            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            use_mixup = random.random() < args.mixup_p; use_cutmix = (not use_mixup) and (random.random() < args.cutmix_p)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.amp.autocast(device_type=device.type, enabled=(device.type==\"cuda\")):\n                if use_mixup:\n                    x_mix, y_a, y_b, lam = mixup_data(x, y, alpha=args.mix_alpha)\n                    logits = model(x_mix); loss = mixup_criterion(criterion, logits, y_a, y_b, lam); y_for_acc = y_a\n                elif use_cutmix:\n                    lam = np.random.beta(args.mix_alpha, args.mix_alpha); idx_perm = torch.randperm(x.size(0), device=x.device)\n                    y_a, y_b = y, y[idx_perm]; W, H = x.shape[3], x.shape[2]; x1, y1, x2, y2 = rand_bbox(W, H, lam)\n                    x[:, :, y1:y2, x1:x2] = x[idx_perm, :, y1:y2, x1:x2]; lam_adj = 1 - ((x2 - x1) * (y2 - y1) / (W * H))\n                    logits = model(x); loss = mixup_criterion(criterion, logits, y_a, y_b, lam_adj); y_for_acc = y_a\n                else: logits = model(x); loss = criterion(logits, y); y_for_acc = y\n            scaler.scale(loss).backward(); scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n            scaler.step(optimizer); scaler.update(); ema.update(model); scheduler.step(); global_step += 1\n            bs = y.size(0); tr_loss_sum += loss.item() * bs; tr_correct += (logits.detach().argmax(1) == y_for_acc).sum().item(); n_samples += bs\n            pbar.set_postfix(loss=tr_loss_sum/max(1,n_samples), acc=tr_correct/max(1,n_samples))\n        tr_loss = tr_loss_sum / max(1, n_samples); tr_acc  = tr_correct / max(1, n_samples)\n        state_backup = {k: v.detach().clone() for k, v in model.state_dict().items()}\n        model.load_state_dict(ema.shadow, strict=True)\n        val_loss, val_acc, prec, rec, f1, val_auc, cm, y_true, y_prob_pos = evaluate(model, val_dl, device, pos_index=idx_fake, tta=args.tta)\n        model.load_state_dict(state_backup, strict=True)\n        \n        # We now save optimizer and scheduler states for even better resumption next time\n        ckpt = {\n            \"model\": ema.shadow, \"epoch\": epoch, \"val_auc\": float(val_auc), \n            \"class_to_idx\": train_ds.class_to_idx, \"args\": args_dict,\n            \"optimizer\": optimizer.state_dict(), \"scheduler\": scheduler.state_dict()\n        }\n        torch.save(ckpt, ckpt_dir / f\"resnet50_epoch{epoch:02d}.pt\")\n        # Overwrite a 'latest' file for easy resuming\n        torch.save(ckpt, ckpt_dir / \"resnet50_latest.pt\")\n        \n        history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc); history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc); history[\"val_auc\"].append(val_auc); history[\"val_f1\"].append(f1)\n        tqdm.write(f\"Epoch {epoch:02d} | tr Ls {tr_loss:.4f} Ac {tr_acc:.4f} | val Ls {val_loss:.4f} Ac {val_acc:.4f} AUC {val_auc:.4f} F1 {f1:.4f}\")\n        class_names = [k for k,_ in sorted(train_ds.class_to_idx.items(), key=lambda kv: kv[1])]\n        plot_cm(cm, art_dir, class_names=class_names)\n        y_true_bin = (y_true == idx_fake).astype(int)\n        _ = plot_roc_and_save(y_true_bin, y_prob_pos, art_dir, pos_label_name=\"fake\")\n        if not math.isnan(val_auc) and val_auc > best_auc:\n            best_auc, best_epoch = val_auc, epoch; torch.save(ckpt, ckpt_dir/\"resnet50_best.pt\")\n            tqdm.write(f\"  >>> New Best AUC: {best_auc:.4f} saved.\")\n    plot_curves(history, art_dir);\n    with open(out_dir/\"history.json\",\"w\") as f: json.dump(history, f, indent=2)\n    print(f\"\\nTraining Complete.\\nBest epoch by AUC: {best_epoch} (AUC={best_auc:.4f})\\nCheckpoints: {ckpt_dir}\\nArtifacts: {art_dir}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:15:39.303531Z","iopub.execute_input":"2025-10-11T08:15:39.303973Z","iopub.status.idle":"2025-10-11T08:53:02.203336Z","shell.execute_reply.started":"2025-10-11T08:15:39.303952Z","shell.execute_reply":"2025-10-11T08:53:02.202487Z"}},"outputs":[{"name":"stdout","text":"\n[Kaggle Setup]\nGPU Detected: Tesla T4\nSeed set to 42.\n\nSaving outputs to: /kaggle/working/training_results\nLoading IMAGE data from: /kaggle/input/faceforensics/WD_subset_png\nLoading VIDEO data from: /kaggle/input/wilddeepfake/ffpp_subset_c23\n\nCombined 2 dataset(s). Training samples: 47927, Validation samples: 5219\nBuilding sequence-limited sampler for combined dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Indexing sequences:   0%|          | 0/47927 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f072f6573c1493eb895c4deba18b6c6"}},"metadata":{}},{"name":"stdout","text":"Master Classes: {'fake': 0, 'real': 1}\nComputing class weights for combined dataset...\nClass counts in one epoch: [1211 1530]. Weights: [1.1317093  0.89575166]\n\nInitializing ResNet-50 model...\n\n[Resuming Training]\n  Fast-forwarding LR scheduler to step of epoch 40...\n  --> Checkpoint loaded. Resuming from Epoch 41. Previous best AUC: 0.8908\n\nStarting Training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 41/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 41 | tr Ls 0.2965 Ac 0.8548 | val Ls 0.5404 Ac 0.7007 AUC 0.8928 F1 0.7316\n  >>> New Best AUC: 0.8928 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 42/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 42 | tr Ls 0.1863 Ac 0.8599 | val Ls 0.5336 Ac 0.7078 AUC 0.8958 F1 0.7363\n  >>> New Best AUC: 0.8958 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 43/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 43 | tr Ls 0.1977 Ac 0.8792 | val Ls 0.5273 Ac 0.7143 AUC 0.8988 F1 0.7406\n  >>> New Best AUC: 0.8988 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 44/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 44 | tr Ls 0.2207 Ac 0.9511 | val Ls 0.5215 Ac 0.7204 AUC 0.9013 F1 0.7447\n  >>> New Best AUC: 0.9013 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 45/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 45 | tr Ls 0.1769 Ac 0.8906 | val Ls 0.5145 Ac 0.7275 AUC 0.9044 F1 0.7496\n  >>> New Best AUC: 0.9044 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 46/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 46 | tr Ls 0.2099 Ac 0.9281 | val Ls 0.5083 Ac 0.7363 AUC 0.9067 F1 0.7557\n  >>> New Best AUC: 0.9067 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 47/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 47 | tr Ls 0.1984 Ac 0.8913 | val Ls 0.5000 Ac 0.7455 AUC 0.9100 F1 0.7621\n  >>> New Best AUC: 0.9100 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 48/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 48 | tr Ls 0.1949 Ac 0.9004 | val Ls 0.4925 Ac 0.7519 AUC 0.9125 F1 0.7665\n  >>> New Best AUC: 0.9125 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 49/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 49 | tr Ls 0.2008 Ac 0.9383 | val Ls 0.4862 Ac 0.7565 AUC 0.9150 F1 0.7697\n  >>> New Best AUC: 0.9150 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 50/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 50 | tr Ls 0.1775 Ac 0.9351 | val Ls 0.4805 Ac 0.7601 AUC 0.9166 F1 0.7720\n  >>> New Best AUC: 0.9166 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 51/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 51 | tr Ls 0.1899 Ac 0.9584 | val Ls 0.4751 Ac 0.7662 AUC 0.9179 F1 0.7765\n  >>> New Best AUC: 0.9179 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 52/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 52 | tr Ls 0.2185 Ac 0.8854 | val Ls 0.4700 Ac 0.7701 AUC 0.9190 F1 0.7789\n  >>> New Best AUC: 0.9190 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 53/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 53 | tr Ls 0.2573 Ac 0.8822 | val Ls 0.4655 Ac 0.7749 AUC 0.9203 F1 0.7824\n  >>> New Best AUC: 0.9203 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 54/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 54 | tr Ls 0.2191 Ac 0.9033 | val Ls 0.4611 Ac 0.7783 AUC 0.9214 F1 0.7847\n  >>> New Best AUC: 0.9214 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 55/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 55 | tr Ls 0.1617 Ac 0.9347 | val Ls 0.4561 Ac 0.7821 AUC 0.9224 F1 0.7870\n  >>> New Best AUC: 0.9224 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 56/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 56 | tr Ls 0.1907 Ac 0.9424 | val Ls 0.4518 Ac 0.7856 AUC 0.9229 F1 0.7895\n  >>> New Best AUC: 0.9229 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 57/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 57 | tr Ls 0.2312 Ac 0.9161 | val Ls 0.4477 Ac 0.7877 AUC 0.9236 F1 0.7907\n  >>> New Best AUC: 0.9236 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 58/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 58 | tr Ls 0.2095 Ac 0.8738 | val Ls 0.4430 Ac 0.7908 AUC 0.9245 F1 0.7929\n  >>> New Best AUC: 0.9245 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 59/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 59 | tr Ls 0.2277 Ac 0.8599 | val Ls 0.4384 Ac 0.7956 AUC 0.9254 F1 0.7963\n  >>> New Best AUC: 0.9254 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 60/60:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 60 | tr Ls 0.2316 Ac 0.8741 | val Ls 0.4340 Ac 0.7980 AUC 0.9262 F1 0.7979\n  >>> New Best AUC: 0.9262 saved.\n\nTraining Complete.\nBest epoch by AUC: 60 (AUC=0.9262)\nCheckpoints: /kaggle/working/training_results/checkpoints\nArtifacts: /kaggle/working/training_results/artifacts\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\"\"\"\nResNet-50 Deepfake Detector - Kaggle GPU Optimized\nVERSION FOR COMBINED IMAGE & VIDEO DATASETS WITH RESUME CAPABILITY\n\"\"\"\n\nimport argparse, io, json, math, os, random, time, sys\nfrom pathlib import Path\nfrom collections import defaultdict\n\n# --- [SETUP] Install necessary libraries ---\ntry:\n    import cv2\nexcept ImportError:\n    print(\"OpenCV not found. Installing opencv-python-headless...\")\n    import subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"opencv-python-headless\"])\n    import cv2\n\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, SubsetRandomSampler, ConcatDataset\nfrom torchvision import datasets, transforms, models\n\nfrom sklearn.metrics import (\n    accuracy_score, precision_recall_fscore_support,\n    confusion_matrix, roc_auc_score, roc_curve\n)\n\n# ----------------------- Utilities & Repro -----------------------\ndef set_seed(seed: int):\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed); torch.backends.cudnn.benchmark = True\n    print(f\"Seed set to {seed}.\")\n\n# ----------------------- Social-style Augmentations -----------------------\nclass RandomJPEGCompression:\n    def __init__(self, qmin=35, qmax=92, p=0.7): self.qmin, self.qmax, self.p = qmin, qmax, p\n    def __call__(self, img: Image.Image) -> Image.Image:\n        if random.random() > self.p: return img\n        buf = io.BytesIO(); img.save(buf, format=\"JPEG\", quality=random.randint(self.qmin, self.qmax))\n        buf.seek(0); return Image.open(buf).convert(\"RGB\")\nclass RandomDownscale:\n    def __init__(self, scale_min=0.4, scale_max=0.85, p=0.6): self.scale_min, self.scale_max, self.p = scale_min, scale_max, p\n    def __call__(self, img: Image.Image) -> Image.Image:\n        if random.random() > self.p: return img\n        w, h = img.size; s = random.uniform(self.scale_min, self.scale_max)\n        nw, nh = max(8, int(w*s)), max(8, int(h*s))\n        img = img.resize((nw, nh), resample=Image.BILINEAR); img = img.resize((w, h), resample=Image.BILINEAR)\n        return img\nclass RandomGaussianNoise:\n    def __init__(self, sigma_min=0.0, sigma_max=0.03, p=0.4): self.sigma_min, self.sigma_max, self.p = sigma_min, sigma_max, p\n    def __call__(self, img: Image.Image) -> Image.Image:\n        if random.random() > self.p: return img\n        arr = np.asarray(img).astype(np.float32)/255.0; sigma = random.uniform(self.sigma_min, self.sigma_max)\n        noise = np.random.normal(0.0, sigma, arr.shape).astype(np.float32); arr = np.clip(arr + noise, 0.0, 1.0)\n        arr = (arr*255.0 + 0.5).astype(np.uint8); return Image.fromarray(arr)\n\n# ----------------------- Custom Video Dataset -----------------------\nclass VideoFrameDataset(torch.utils.data.Dataset):\n    def __init__(self, root: Path, transform=None, class_to_idx=None):\n        self.root = root; self.transform = transform; self.class_to_idx = class_to_idx\n        self.samples = []; self.targets = []\n        for class_name in self.class_to_idx.keys():\n            class_dir = self.root / class_name\n            if not class_dir.exists(): continue\n            for video_path in sorted(list(class_dir.rglob('*.mp4'))):\n                label = self.class_to_idx[class_name]\n                self.samples.append((str(video_path), label)); self.targets.append(label)\n        if not self.samples: print(f\"  --> WARNING: No .mp4 videos found in {self.root}\")\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, idx):\n        video_path, label = self.samples[idx]\n        cap = None\n        try:\n            cap = cv2.VideoCapture(video_path)\n            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n            if total_frames < 1: return self.__getitem__((idx + 1) % len(self))\n            frame_idx = random.randint(0, total_frames - 1)\n            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n            ret, frame = cap.read()\n            if not ret: return self.__getitem__((idx + 1) % len(self))\n        except Exception as e:\n            print(f\"  --> Error processing {video_path}: {e}. Skipping.\"); return self.__getitem__((idx + 1) % len(self))\n        finally:\n            if cap: cap.release()\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB); img = Image.fromarray(frame_rgb)\n        if self.transform: img = self.transform(img)\n        return img, label\n\n# ----------------------- Data Loading Pipeline -----------------------\ndef build_indices_limit_per_seq(dataset: ConcatDataset, max_per_seq=60, seed=42):\n    if max_per_seq is None or max_per_seq <= 0: return list(range(len(dataset)))\n    by_seq = defaultdict(list)\n    print(\"Building sequence-limited sampler for combined dataset...\")\n    for idx, (path, _) in enumerate(tqdm(dataset.samples, desc=\"Indexing sequences\")):\n        path_obj = Path(path)\n        seq_id = path_obj.stem if path_obj.suffix.lower() == '.mp4' else path_obj.parent.name\n        by_seq[seq_id].append(idx)\n    rng = random.Random(seed); indices = []\n    for seq, idxs in by_seq.items(): rng.shuffle(idxs); indices.extend(idxs[:max_per_seq])\n    rng.shuffle(indices); return indices\n\ndef build_loaders(image_data_root, video_data_root, img_size=224, batch_size=64, workers=2, per_seq_cap=60):\n    normalize = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n    train_tf = transforms.Compose([\n        transforms.Resize(int(img_size*1.2)), transforms.RandomResizedCrop(img_size, scale=(0.55, 1.0), ratio=(0.75, 1.33)),\n        transforms.RandomHorizontalFlip(p=0.5), transforms.RandomRotation(degrees=10),\n        transforms.RandomPerspective(distortion_scale=0.25, p=0.25), RandomJPEGCompression(qmin=35, qmax=92, p=0.7),\n        RandomDownscale(scale_min=0.4, scale_max=0.85, p=0.6), transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.5)),\n        transforms.ColorJitter(0.15, 0.15, 0.15, 0.05), transforms.RandomAdjustSharpness(sharpness_factor=0.6, p=0.3),\n        transforms.RandomAutocontrast(p=0.3), RandomGaussianNoise(sigma_min=0.0, sigma_max=0.03, p=0.4),\n        transforms.ToTensor(), normalize, transforms.RandomErasing(p=0.25, scale=(0.02,0.2), ratio=(0.3,3.3), value='random'),\n    ])\n    val_tf = transforms.Compose([transforms.Resize(int(img_size*1.15)), transforms.CenterCrop(img_size), transforms.ToTensor(), normalize])\n    train_datasets, val_datasets = [], []; master_class_to_idx = {'fake': 0, 'real': 1}\n    if image_data_root:\n        root = Path(image_data_root); print(f\"Loading IMAGE data from: {root}\")\n        if (root/\"train\").exists() and (root/\"val\").exists():\n            train_datasets.append(datasets.ImageFolder(root/\"train\", transform=train_tf))\n            val_datasets.append(datasets.ImageFolder(root/\"val\", transform=val_tf))\n        else: print(f\"  --> WARNING: 'train' or 'val' not found in {root}. Skipping.\")\n    if video_data_root:\n        root = Path(video_data_root); print(f\"Loading VIDEO data from: {root}\")\n        if (root/\"train\").exists() and (root/\"val\").exists():\n            train_datasets.append(VideoFrameDataset(root/\"train\", transform=train_tf, class_to_idx=master_class_to_idx))\n            val_datasets.append(VideoFrameDataset(root/\"val\", transform=val_tf, class_to_idx=master_class_to_idx))\n        else: print(f\"  --> WARNING: 'train' or 'val' not found in {root}. Skipping.\")\n    if not train_datasets: raise ValueError(\"No valid datasets were loaded. Check paths in USER CONFIGURATION.\")\n    train_ds = ConcatDataset(train_datasets); val_ds = ConcatDataset(val_datasets)\n    train_ds.class_to_idx = master_class_to_idx; train_ds.classes = list(master_class_to_idx.keys())\n    train_ds.samples = [s for ds in train_ds.datasets for s in ds.samples]\n    train_ds.targets = [t for ds in train_ds.datasets for t in ds.targets]\n    val_ds.targets = [t for ds in val_ds.datasets for t in ds.targets]\n    print(f\"\\nCombined {len(train_datasets)} dataset(s). Training samples: {len(train_ds)}, Validation samples: {len(val_ds)}\")\n    indices = build_indices_limit_per_seq(train_ds, max_per_seq=per_seq_cap, seed=42)\n    num_workers = 2 if os.cpu_count() <= 4 else 4\n    train_dl = DataLoader(train_ds, batch_size=batch_size, sampler=SubsetRandomSampler(indices), num_workers=num_workers, pin_memory=True, persistent_workers=(num_workers>0))\n    val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, persistent_workers=(num_workers>0))\n    return train_ds, val_ds, train_dl, val_dl\n\n# --- [The rest of the helper functions are unchanged] ---\nclass WarmupThenCosine(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self, optimizer, warmup_steps, total_steps, last_epoch=-1): self.warmup_steps=max(1,warmup_steps); self.total_steps=max(self.warmup_steps+1,total_steps); super().__init__(optimizer, last_epoch)\n    def get_lr(self):\n        step = self.last_epoch + 1\n        if step <= self.warmup_steps: return [base * (step/self.warmup_steps) for base in self.base_lrs]\n        t = (step-self.warmup_steps)/(self.total_steps-self.warmup_steps); return [base*0.5*(1+math.cos(math.pi*t)) for base in self.base_lrs]\nclass EMA:\n    def __init__(self, model, decay=0.999): self.decay = decay; self.shadow = {k: v.detach().clone() for k, v in model.state_dict().items()}\n    @torch.no_grad()\n    def update(self, model):\n        for k, v in model.state_dict().items():\n            if not v.is_floating_point(): self.shadow[k] = v.detach().clone(); continue\n            if (self.shadow[k].dtype!=v.dtype)or(self.shadow[k].device!=v.device): self.shadow[k]=v.detach().clone(); continue\n            self.shadow[k].mul_(self.decay).add_(v.detach(), alpha=1.0 - self.decay)\ndef rand_bbox(W, H, lam):\n    cut_w=int(W*math.sqrt(1-lam)); cut_h=int(H*math.sqrt(1-lam)); cx,cy=random.randint(0,W-1),random.randint(0,H-1)\n    x1,y1=max(cx-cut_w//2,0),max(cy-cut_h//2,0); x2,y2=min(cx+cut_w//2,W),min(cy+cut_h//2,H); return x1,y1,x2,y2\ndef mixup_data(x, y, alpha=0.2):\n    lam=np.random.beta(alpha,alpha) if alpha>0 else 1.0; idx=torch.randperm(x.size(0),device=x.device)\n    mixed_x=lam*x+(1-lam)*x[idx]; y_a,y_b=y,y[idx]; return mixed_x,y_a,y_b,lam\ndef mixup_criterion(criterion, pred, y_a, y_b, lam): return lam*criterion(pred,y_a)+(1-lam)*criterion(pred,y_b)\n@torch.no_grad()\ndef tta_logits(model,x,device,tta=4):\n    outs=[]\n    for op in range(tta):\n        xx=x;\n        if op%2==1: xx=torch.flip(xx,dims=[-1])\n        with torch.amp.autocast(device_type=device.type, enabled=(device.type==\"cuda\")): outs.append(model(xx))\n    return torch.stack(outs,0).mean(0)\n@torch.no_grad()\ndef evaluate(model,dl,device,pos_index,tta=4):\n    model.eval(); y_true,y_pred,y_prob_pos=[],[],[]; total_loss,n=0.0,0\n    criterion=nn.CrossEntropyLoss()\n    for x,y in dl:\n        x,y=x.to(device,non_blocking=True),y.to(device,non_blocking=True)\n        logits=tta_logits(model,x,device,tta=tta) if tta and tta>1 else model(x)\n        with torch.amp.autocast(device_type=device.type,enabled=(device.type==\"cuda\")): loss=criterion(logits,y)\n        prob=torch.softmax(logits,dim=1)\n        y_true.extend(y.cpu().tolist());y_pred.extend(prob.argmax(1).cpu().tolist());y_prob_pos.extend(prob[:,pos_index].cpu().tolist())\n        bs=y.size(0);total_loss+=loss.item()*bs;n+=bs\n    acc=accuracy_score(y_true,y_pred); prec,rec,f1,_=precision_recall_fscore_support(y_true,y_pred,average=\"binary\",pos_label=pos_index,zero_division=0)\n    try: auc=roc_auc_score([1 if t==pos_index else 0 for t in y_true], y_prob_pos)\n    except Exception: auc=float(\"nan\")\n    cm=confusion_matrix(y_true,y_pred,labels=[0,1]); return (total_loss/n,acc,prec,rec,f1,auc,np.array(cm),np.array(y_true),np.array(y_prob_pos))\ndef plot_curves(history,out_dir:Path):\n    out_dir.mkdir(parents=True,exist_ok=True);epochs=np.arange(1,len(history[\"train_loss\"])+1)\n    plt.figure();plt.plot(epochs,history[\"train_loss\"],label=\"train loss\");plt.plot(epochs,history[\"val_loss\"],label=\"val loss\");plt.xlabel(\"epoch\");plt.ylabel(\"loss\");plt.legend();plt.tight_layout();plt.savefig(out_dir/\"loss_curves.png\");plt.close()\n    plt.figure();plt.plot(epochs,history[\"train_acc\"],label=\"train acc\");plt.plot(epochs,history[\"val_acc\"],label=\"val acc\");plt.xlabel(\"epoch\");plt.ylabel(\"accuracy\");plt.legend();plt.tight_layout();plt.savefig(out_dir/\"acc_curves.png\");plt.close()\ndef plot_cm(cm,out_dir:Path,class_names):\n    plt.figure();plt.imshow(cm,interpolation=\"nearest\");plt.title(\"Confusion Matrix\");plt.colorbar();ticks=np.arange(len(class_names));plt.xticks(ticks,class_names,rotation=45);plt.yticks(ticks,class_names);thresh=cm.max()/2.0\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]): plt.text(j,i,int(cm[i,j]),ha=\"center\",va=\"center\",color=\"white\" if cm[i,j]>thresh else \"black\")\n    plt.ylabel(\"True\");plt.xlabel(\"Predicted\");plt.tight_layout();plt.savefig(out_dir/\"confusion_matrix.png\");plt.close()\ndef plot_roc_and_save(y_true_bin,y_prob_pos,out_dir:Path,pos_label_name=\"fake\"):\n    fpr,tpr,_=roc_curve(y_true_bin,y_prob_pos);auc=roc_auc_score(y_true_bin,y_prob_pos)\n    plt.figure();plt.plot(fpr,tpr,label=f\"AUC={auc:.3f}\");plt.plot([0,1],[0,1],\"--\");plt.xlabel(\"FPR\");plt.ylabel(\"TPR\");plt.title(f\"ROC ({pos_label_name} positive)\");plt.legend();plt.tight_layout();plt.savefig(out_dir/\"roc_curve.png\");plt.close();return auc\n\n# ----------------------- Main -----------------------\ndef main():\n    \n    # =================================================================================\n    # ===> KAGGLE USER CONFIGURATION SECTION (CRITICAL!) <===\n    USER_IMAGE_DATA_ROOT = \"/kaggle/input/faceforensics/WD_subset_png\"\n    USER_VIDEO_DATA_ROOT = \"/kaggle/input/wilddeepfake/ffpp_subset_c23\"\n    USER_OUT_DIR = \"/kaggle/working/training_results\"\n\n    # ===> NEW: RESUME TRAINING CONFIGURATION <===\n    RESUME_TRAINING = True # SET THIS TO True TO RESUME\n    # This path points to the 'latest' checkpoint inside your output directory\n    CHECKPOINT_PATH = f\"{USER_OUT_DIR}/checkpoints/resnet50_epoch60.pt\"\n    # =================================================================================\n\n    args_dict = {\n        \"image_data_root\": USER_IMAGE_DATA_ROOT, \"video_data_root\": USER_VIDEO_DATA_ROOT,\n        \"out_dir\": USER_OUT_DIR, \"per_seq_cap\": 60, \n        \"epochs\": 80, # <== INCREASE THE TOTAL NUMBER OF EPOCHS\n        \"batch_size\": 128, \"img_size\": 224, \"workers\": 2,\n        \"lr\": 3e-4, \"weight_decay\": 1e-4, \"label_smoothing\": 0.05, \"freeze_backbone\": False,\n        \"class_weights\": True, \"mixup_p\": 0.3, \"cutmix_p\": 0.2, \"mix_alpha\": 0.2,\n        \"ema_decay\": 0.999, \"grad_clip\": 1.0, \"warmup_pct\": 0.1, \"tta\": 4, \"seed\": 42,\n    }\n    args = argparse.Namespace(**args_dict)\n\n    print(\"\\n[Kaggle Setup]\");\n    if not torch.cuda.is_available(): print(\"\\n!!!! WARNING: GPU ACCELERATOR NOT DETECTED !!!!\\n\")\n    else: print(f\"GPU Detected: {torch.cuda.get_device_name(0)}\")\n\n    set_seed(args.seed); device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); out_dir = Path(args.out_dir)\n    ckpt_dir = out_dir / \"checkpoints\"; art_dir = out_dir / \"artifacts\"\n    ckpt_dir.mkdir(parents=True, exist_ok=True); art_dir.mkdir(parents=True, exist_ok=True)\n    print(f\"\\nSaving outputs to: {out_dir}\")\n\n    train_ds, val_ds, train_dl, val_dl = build_loaders(\n        image_data_root=args.image_data_root, video_data_root=args.video_data_root,\n        img_size=args.img_size, batch_size=args.batch_size, workers=args.workers, per_seq_cap=args.per_seq_cap\n    )\n    print(f\"Master Classes: {train_ds.class_to_idx}\"); idx_fake = train_ds.class_to_idx.get(\"fake\", 0)\n\n    class_weights_tensor = None\n    if args.class_weights:\n        print(\"Computing class weights for combined dataset...\")\n        sampler_indices = list(train_dl.sampler.indices); labels = [train_ds.targets[i] for i in sampler_indices]\n        counts = np.bincount(labels, minlength=len(train_ds.classes)); total = counts.sum()\n        weights = [total/(len(counts)*c) if c > 0 else 0.0 for c in counts]\n        class_weights_tensor = torch.tensor(weights, dtype=torch.float32, device=device)\n        print(f\"Class counts in one epoch: {counts}. Weights: {class_weights_tensor.cpu().numpy()}\")\n\n    print(\"\\nInitializing ResNet-50 model...\"); model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n    model.fc = nn.Linear(model.fc.in_features, len(train_ds.classes))\n    if args.freeze_backbone: [p.requires_grad_(False) for n, p in model.named_parameters() if not n.startswith(\"fc.\")]\n    model.to(device)\n    \n    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=args.label_smoothing)\n    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, weight_decay=args.weight_decay)\n    total_steps = len(train_dl) * args.epochs; warmup_steps = int(args.warmup_pct * total_steps)\n    scheduler = WarmupThenCosine(optimizer, warmup_steps=warmup_steps, total_steps=total_steps)\n    scaler = torch.amp.GradScaler(enabled=(device.type==\"cuda\"))\n    ema = EMA(model, decay=args.ema_decay)\n    history = defaultdict(list); best_auc, best_epoch = -1.0, -1; global_step = 0; start_epoch = 1\n\n    # ===> NEW: LOGIC TO LOAD CHECKPOINT AND RESUME <===\n    if RESUME_TRAINING:\n        print(\"\\n[Resuming Training]\")\n        if not Path(CHECKPOINT_PATH).exists():\n            print(f\"  !!! WARNING: Checkpoint not found at {CHECKPOINT_PATH}. Starting from scratch. !!!\")\n        else:\n            # For modern PyTorch, weights_only=False is needed for non-tensor data\n            ckpt = torch.load(CHECKPOINT_PATH, map_location='cpu', weights_only=False)\n            \n            # The saved model is the EMA model\n            model.load_state_dict(ckpt['model'], strict=True) \n            model.to(device)\n            ema.shadow = {k: v.to(device) for k, v in ckpt['model'].items()}\n            \n            start_epoch = ckpt.get('epoch', 0) + 1\n            \n            print(f\"  Fast-forwarding LR scheduler to step of epoch {start_epoch - 1}...\")\n            steps_to_advance = (start_epoch - 1) * len(train_dl)\n            for _ in range(steps_to_advance): scheduler.step()\n\n            history_path = out_dir / \"history.json\"\n            if history_path.exists():\n                with open(history_path, 'r') as f: history = defaultdict(list, json.load(f))\n                if history.get('val_auc'): best_auc = max(history.get('val_auc', [0.0]))\n            print(f\"  --> Checkpoint loaded. Resuming from Epoch {start_epoch}. Previous best AUC: {best_auc:.4f}\")\n\n    print(\"\\nStarting Training...\")\n    # The loop now starts from 'start_epoch'\n    for epoch in range(start_epoch, args.epochs + 1):\n        model.train(); tr_loss_sum, tr_correct, n_samples = 0.0, 0, 0\n        pbar = tqdm(train_dl, desc=f\"Epoch {epoch}/{args.epochs}\", leave=False)\n        for x, y in pbar:\n            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            use_mixup = random.random() < args.mixup_p; use_cutmix = (not use_mixup) and (random.random() < args.cutmix_p)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.amp.autocast(device_type=device.type, enabled=(device.type==\"cuda\")):\n                if use_mixup:\n                    x_mix, y_a, y_b, lam = mixup_data(x, y, alpha=args.mix_alpha)\n                    logits = model(x_mix); loss = mixup_criterion(criterion, logits, y_a, y_b, lam); y_for_acc = y_a\n                elif use_cutmix:\n                    lam = np.random.beta(args.mix_alpha, args.mix_alpha); idx_perm = torch.randperm(x.size(0), device=x.device)\n                    y_a, y_b = y, y[idx_perm]; W, H = x.shape[3], x.shape[2]; x1, y1, x2, y2 = rand_bbox(W, H, lam)\n                    x[:, :, y1:y2, x1:x2] = x[idx_perm, :, y1:y2, x1:x2]; lam_adj = 1 - ((x2 - x1) * (y2 - y1) / (W * H))\n                    logits = model(x); loss = mixup_criterion(criterion, logits, y_a, y_b, lam_adj); y_for_acc = y_a\n                else: logits = model(x); loss = criterion(logits, y); y_for_acc = y\n            scaler.scale(loss).backward(); scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n            scaler.step(optimizer); scaler.update(); ema.update(model); scheduler.step(); global_step += 1\n            bs = y.size(0); tr_loss_sum += loss.item() * bs; tr_correct += (logits.detach().argmax(1) == y_for_acc).sum().item(); n_samples += bs\n            pbar.set_postfix(loss=tr_loss_sum/max(1,n_samples), acc=tr_correct/max(1,n_samples))\n        tr_loss = tr_loss_sum / max(1, n_samples); tr_acc  = tr_correct / max(1, n_samples)\n        state_backup = {k: v.detach().clone() for k, v in model.state_dict().items()}\n        model.load_state_dict(ema.shadow, strict=True)\n        val_loss, val_acc, prec, rec, f1, val_auc, cm, y_true, y_prob_pos = evaluate(model, val_dl, device, pos_index=idx_fake, tta=args.tta)\n        model.load_state_dict(state_backup, strict=True)\n        \n        # We now save optimizer and scheduler states for even better resumption next time\n        ckpt = {\n            \"model\": ema.shadow, \"epoch\": epoch, \"val_auc\": float(val_auc), \n            \"class_to_idx\": train_ds.class_to_idx, \"args\": args_dict,\n            \"optimizer\": optimizer.state_dict(), \"scheduler\": scheduler.state_dict()\n        }\n        torch.save(ckpt, ckpt_dir / f\"resnet50_epoch{epoch:02d}.pt\")\n        # Overwrite a 'latest' file for easy resuming\n        torch.save(ckpt, ckpt_dir / \"resnet50_latest.pt\")\n        \n        history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc); history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc); history[\"val_auc\"].append(val_auc); history[\"val_f1\"].append(f1)\n        tqdm.write(f\"Epoch {epoch:02d} | tr Ls {tr_loss:.4f} Ac {tr_acc:.4f} | val Ls {val_loss:.4f} Ac {val_acc:.4f} AUC {val_auc:.4f} F1 {f1:.4f}\")\n        class_names = [k for k,_ in sorted(train_ds.class_to_idx.items(), key=lambda kv: kv[1])]\n        plot_cm(cm, art_dir, class_names=class_names)\n        y_true_bin = (y_true == idx_fake).astype(int)\n        _ = plot_roc_and_save(y_true_bin, y_prob_pos, art_dir, pos_label_name=\"fake\")\n        if not math.isnan(val_auc) and val_auc > best_auc:\n            best_auc, best_epoch = val_auc, epoch; torch.save(ckpt, ckpt_dir/\"resnet50_best.pt\")\n            tqdm.write(f\"  >>> New Best AUC: {best_auc:.4f} saved.\")\n    plot_curves(history, art_dir);\n    with open(out_dir/\"history.json\",\"w\") as f: json.dump(history, f, indent=2)\n    print(f\"\\nTraining Complete.\\nBest epoch by AUC: {best_epoch} (AUC={best_auc:.4f})\\nCheckpoints: {ckpt_dir}\\nArtifacts: {art_dir}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:53:20.519027Z","iopub.execute_input":"2025-10-11T08:53:20.519861Z","iopub.status.idle":"2025-10-11T09:30:28.552038Z","shell.execute_reply.started":"2025-10-11T08:53:20.519833Z","shell.execute_reply":"2025-10-11T09:30:28.551102Z"}},"outputs":[{"name":"stdout","text":"\n[Kaggle Setup]\nGPU Detected: Tesla T4\nSeed set to 42.\n\nSaving outputs to: /kaggle/working/training_results\nLoading IMAGE data from: /kaggle/input/faceforensics/WD_subset_png\nLoading VIDEO data from: /kaggle/input/wilddeepfake/ffpp_subset_c23\n\nCombined 2 dataset(s). Training samples: 47927, Validation samples: 5219\nBuilding sequence-limited sampler for combined dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Indexing sequences:   0%|          | 0/47927 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7426becea9bc4835a6cd9dd19409d001"}},"metadata":{}},{"name":"stdout","text":"Master Classes: {'fake': 0, 'real': 1}\nComputing class weights for combined dataset...\nClass counts in one epoch: [1211 1530]. Weights: [1.1317093  0.89575166]\n\nInitializing ResNet-50 model...\n\n[Resuming Training]\n  Fast-forwarding LR scheduler to step of epoch 60...\n  --> Checkpoint loaded. Resuming from Epoch 61. Previous best AUC: 0.9262\n\nStarting Training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 61/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 61 | tr Ls 0.2603 Ac 0.8709 | val Ls 0.4309 Ac 0.7994 AUC 0.9269 F1 0.7988\n  >>> New Best AUC: 0.9269 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 62/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 62 | tr Ls 0.1741 Ac 0.8621 | val Ls 0.4266 Ac 0.8023 AUC 0.9276 F1 0.8006\n  >>> New Best AUC: 0.9276 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 63/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 63 | tr Ls 0.1873 Ac 0.8822 | val Ls 0.4213 Ac 0.8046 AUC 0.9286 F1 0.8019\n  >>> New Best AUC: 0.9286 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 64/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 64 | tr Ls 0.2106 Ac 0.9570 | val Ls 0.4180 Ac 0.8080 AUC 0.9287 F1 0.8044\n  >>> New Best AUC: 0.9287 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 65/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 65 | tr Ls 0.1690 Ac 0.8924 | val Ls 0.4147 Ac 0.8103 AUC 0.9287 F1 0.8055\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 66/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 66 | tr Ls 0.2003 Ac 0.9314 | val Ls 0.4116 Ac 0.8130 AUC 0.9289 F1 0.8076\n  >>> New Best AUC: 0.9289 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 67/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 67 | tr Ls 0.1914 Ac 0.8924 | val Ls 0.4085 Ac 0.8164 AUC 0.9293 F1 0.8103\n  >>> New Best AUC: 0.9293 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 68/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 68 | tr Ls 0.1898 Ac 0.9011 | val Ls 0.4047 Ac 0.8191 AUC 0.9296 F1 0.8124\n  >>> New Best AUC: 0.9296 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 69/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 69 | tr Ls 0.1962 Ac 0.9427 | val Ls 0.4013 Ac 0.8201 AUC 0.9305 F1 0.8128\n  >>> New Best AUC: 0.9305 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 70/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 70 | tr Ls 0.1749 Ac 0.9351 | val Ls 0.3991 Ac 0.8222 AUC 0.9306 F1 0.8143\n  >>> New Best AUC: 0.9306 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 71/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 71 | tr Ls 0.1878 Ac 0.9606 | val Ls 0.3967 Ac 0.8241 AUC 0.9303 F1 0.8158\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 72/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 72 | tr Ls 0.2176 Ac 0.8865 | val Ls 0.3944 Ac 0.8258 AUC 0.9304 F1 0.8169\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 73/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 73 | tr Ls 0.2561 Ac 0.8811 | val Ls 0.3922 Ac 0.8274 AUC 0.9306 F1 0.8180\n  >>> New Best AUC: 0.9306 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 74/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 74 | tr Ls 0.2181 Ac 0.9037 | val Ls 0.3909 Ac 0.8272 AUC 0.9305 F1 0.8178\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 75/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 75 | tr Ls 0.1615 Ac 0.9347 | val Ls 0.3888 Ac 0.8274 AUC 0.9305 F1 0.8174\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 76/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 76 | tr Ls 0.1909 Ac 0.9435 | val Ls 0.3866 Ac 0.8277 AUC 0.9304 F1 0.8174\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 77/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 77 | tr Ls 0.2320 Ac 0.9190 | val Ls 0.3852 Ac 0.8281 AUC 0.9301 F1 0.8174\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 78/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 78 | tr Ls 0.2092 Ac 0.8734 | val Ls 0.3839 Ac 0.8293 AUC 0.9297 F1 0.8181\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 79/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 79 | tr Ls 0.2281 Ac 0.8548 | val Ls 0.3819 Ac 0.8320 AUC 0.9296 F1 0.8203\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 80/80:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 80 | tr Ls 0.2325 Ac 0.8730 | val Ls 0.3805 Ac 0.8329 AUC 0.9294 F1 0.8208\n\nTraining Complete.\nBest epoch by AUC: 73 (AUC=0.9306)\nCheckpoints: /kaggle/working/training_results/checkpoints\nArtifacts: /kaggle/working/training_results/artifacts\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\"\"\"\nResNet-50 Deepfake Detector - Kaggle GPU Optimized\nVERSION FOR COMBINED IMAGE & VIDEO DATASETS WITH RESUME CAPABILITY\n\"\"\"\n\nimport argparse, io, json, math, os, random, time, sys\nfrom pathlib import Path\nfrom collections import defaultdict\n\n# --- [SETUP] Install necessary libraries ---\ntry:\n    import cv2\nexcept ImportError:\n    print(\"OpenCV not found. Installing opencv-python-headless...\")\n    import subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"opencv-python-headless\"])\n    import cv2\n\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, SubsetRandomSampler, ConcatDataset\nfrom torchvision import datasets, transforms, models\n\nfrom sklearn.metrics import (\n    accuracy_score, precision_recall_fscore_support,\n    confusion_matrix, roc_auc_score, roc_curve\n)\n\n# ----------------------- Utilities & Repro -----------------------\ndef set_seed(seed: int):\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed); torch.backends.cudnn.benchmark = True\n    print(f\"Seed set to {seed}.\")\n\n# ----------------------- Social-style Augmentations -----------------------\nclass RandomJPEGCompression:\n    def __init__(self, qmin=35, qmax=92, p=0.7): self.qmin, self.qmax, self.p = qmin, qmax, p\n    def __call__(self, img: Image.Image) -> Image.Image:\n        if random.random() > self.p: return img\n        buf = io.BytesIO(); img.save(buf, format=\"JPEG\", quality=random.randint(self.qmin, self.qmax))\n        buf.seek(0); return Image.open(buf).convert(\"RGB\")\nclass RandomDownscale:\n    def __init__(self, scale_min=0.4, scale_max=0.85, p=0.6): self.scale_min, self.scale_max, self.p = scale_min, scale_max, p\n    def __call__(self, img: Image.Image) -> Image.Image:\n        if random.random() > self.p: return img\n        w, h = img.size; s = random.uniform(self.scale_min, self.scale_max)\n        nw, nh = max(8, int(w*s)), max(8, int(h*s))\n        img = img.resize((nw, nh), resample=Image.BILINEAR); img = img.resize((w, h), resample=Image.BILINEAR)\n        return img\nclass RandomGaussianNoise:\n    def __init__(self, sigma_min=0.0, sigma_max=0.03, p=0.4): self.sigma_min, self.sigma_max, self.p = sigma_min, sigma_max, p\n    def __call__(self, img: Image.Image) -> Image.Image:\n        if random.random() > self.p: return img\n        arr = np.asarray(img).astype(np.float32)/255.0; sigma = random.uniform(self.sigma_min, self.sigma_max)\n        noise = np.random.normal(0.0, sigma, arr.shape).astype(np.float32); arr = np.clip(arr + noise, 0.0, 1.0)\n        arr = (arr*255.0 + 0.5).astype(np.uint8); return Image.fromarray(arr)\n\n# ----------------------- Custom Video Dataset -----------------------\nclass VideoFrameDataset(torch.utils.data.Dataset):\n    def __init__(self, root: Path, transform=None, class_to_idx=None):\n        self.root = root; self.transform = transform; self.class_to_idx = class_to_idx\n        self.samples = []; self.targets = []\n        for class_name in self.class_to_idx.keys():\n            class_dir = self.root / class_name\n            if not class_dir.exists(): continue\n            for video_path in sorted(list(class_dir.rglob('*.mp4'))):\n                label = self.class_to_idx[class_name]\n                self.samples.append((str(video_path), label)); self.targets.append(label)\n        if not self.samples: print(f\"  --> WARNING: No .mp4 videos found in {self.root}\")\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, idx):\n        video_path, label = self.samples[idx]\n        cap = None\n        try:\n            cap = cv2.VideoCapture(video_path)\n            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n            if total_frames < 1: return self.__getitem__((idx + 1) % len(self))\n            frame_idx = random.randint(0, total_frames - 1)\n            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n            ret, frame = cap.read()\n            if not ret: return self.__getitem__((idx + 1) % len(self))\n        except Exception as e:\n            print(f\"  --> Error processing {video_path}: {e}. Skipping.\"); return self.__getitem__((idx + 1) % len(self))\n        finally:\n            if cap: cap.release()\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB); img = Image.fromarray(frame_rgb)\n        if self.transform: img = self.transform(img)\n        return img, label\n\n# ----------------------- Data Loading Pipeline -----------------------\ndef build_indices_limit_per_seq(dataset: ConcatDataset, max_per_seq=60, seed=42):\n    if max_per_seq is None or max_per_seq <= 0: return list(range(len(dataset)))\n    by_seq = defaultdict(list)\n    print(\"Building sequence-limited sampler for combined dataset...\")\n    for idx, (path, _) in enumerate(tqdm(dataset.samples, desc=\"Indexing sequences\")):\n        path_obj = Path(path)\n        seq_id = path_obj.stem if path_obj.suffix.lower() == '.mp4' else path_obj.parent.name\n        by_seq[seq_id].append(idx)\n    rng = random.Random(seed); indices = []\n    for seq, idxs in by_seq.items(): rng.shuffle(idxs); indices.extend(idxs[:max_per_seq])\n    rng.shuffle(indices); return indices\n\ndef build_loaders(image_data_root, video_data_root, img_size=224, batch_size=64, workers=2, per_seq_cap=60):\n    normalize = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n    train_tf = transforms.Compose([\n        transforms.Resize(int(img_size*1.2)), transforms.RandomResizedCrop(img_size, scale=(0.55, 1.0), ratio=(0.75, 1.33)),\n        transforms.RandomHorizontalFlip(p=0.5), transforms.RandomRotation(degrees=10),\n        transforms.RandomPerspective(distortion_scale=0.25, p=0.25), RandomJPEGCompression(qmin=35, qmax=92, p=0.7),\n        RandomDownscale(scale_min=0.4, scale_max=0.85, p=0.6), transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.5)),\n        transforms.ColorJitter(0.15, 0.15, 0.15, 0.05), transforms.RandomAdjustSharpness(sharpness_factor=0.6, p=0.3),\n        transforms.RandomAutocontrast(p=0.3), RandomGaussianNoise(sigma_min=0.0, sigma_max=0.03, p=0.4),\n        transforms.ToTensor(), normalize, transforms.RandomErasing(p=0.25, scale=(0.02,0.2), ratio=(0.3,3.3), value='random'),\n    ])\n    val_tf = transforms.Compose([transforms.Resize(int(img_size*1.15)), transforms.CenterCrop(img_size), transforms.ToTensor(), normalize])\n    train_datasets, val_datasets = [], []; master_class_to_idx = {'fake': 0, 'real': 1}\n    if image_data_root:\n        root = Path(image_data_root); print(f\"Loading IMAGE data from: {root}\")\n        if (root/\"train\").exists() and (root/\"val\").exists():\n            train_datasets.append(datasets.ImageFolder(root/\"train\", transform=train_tf))\n            val_datasets.append(datasets.ImageFolder(root/\"val\", transform=val_tf))\n        else: print(f\"  --> WARNING: 'train' or 'val' not found in {root}. Skipping.\")\n    if video_data_root:\n        root = Path(video_data_root); print(f\"Loading VIDEO data from: {root}\")\n        if (root/\"train\").exists() and (root/\"val\").exists():\n            train_datasets.append(VideoFrameDataset(root/\"train\", transform=train_tf, class_to_idx=master_class_to_idx))\n            val_datasets.append(VideoFrameDataset(root/\"val\", transform=val_tf, class_to_idx=master_class_to_idx))\n        else: print(f\"  --> WARNING: 'train' or 'val' not found in {root}. Skipping.\")\n    if not train_datasets: raise ValueError(\"No valid datasets were loaded. Check paths in USER CONFIGURATION.\")\n    train_ds = ConcatDataset(train_datasets); val_ds = ConcatDataset(val_datasets)\n    train_ds.class_to_idx = master_class_to_idx; train_ds.classes = list(master_class_to_idx.keys())\n    train_ds.samples = [s for ds in train_ds.datasets for s in ds.samples]\n    train_ds.targets = [t for ds in train_ds.datasets for t in ds.targets]\n    val_ds.targets = [t for ds in val_ds.datasets for t in ds.targets]\n    print(f\"\\nCombined {len(train_datasets)} dataset(s). Training samples: {len(train_ds)}, Validation samples: {len(val_ds)}\")\n    indices = build_indices_limit_per_seq(train_ds, max_per_seq=per_seq_cap, seed=42)\n    num_workers = 2 if os.cpu_count() <= 4 else 4\n    train_dl = DataLoader(train_ds, batch_size=batch_size, sampler=SubsetRandomSampler(indices), num_workers=num_workers, pin_memory=True, persistent_workers=(num_workers>0))\n    val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, persistent_workers=(num_workers>0))\n    return train_ds, val_ds, train_dl, val_dl\n\n# --- [The rest of the helper functions are unchanged] ---\nclass WarmupThenCosine(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self, optimizer, warmup_steps, total_steps, last_epoch=-1): self.warmup_steps=max(1,warmup_steps); self.total_steps=max(self.warmup_steps+1,total_steps); super().__init__(optimizer, last_epoch)\n    def get_lr(self):\n        step = self.last_epoch + 1\n        if step <= self.warmup_steps: return [base * (step/self.warmup_steps) for base in self.base_lrs]\n        t = (step-self.warmup_steps)/(self.total_steps-self.warmup_steps); return [base*0.5*(1+math.cos(math.pi*t)) for base in self.base_lrs]\nclass EMA:\n    def __init__(self, model, decay=0.999): self.decay = decay; self.shadow = {k: v.detach().clone() for k, v in model.state_dict().items()}\n    @torch.no_grad()\n    def update(self, model):\n        for k, v in model.state_dict().items():\n            if not v.is_floating_point(): self.shadow[k] = v.detach().clone(); continue\n            if (self.shadow[k].dtype!=v.dtype)or(self.shadow[k].device!=v.device): self.shadow[k]=v.detach().clone(); continue\n            self.shadow[k].mul_(self.decay).add_(v.detach(), alpha=1.0 - self.decay)\ndef rand_bbox(W, H, lam):\n    cut_w=int(W*math.sqrt(1-lam)); cut_h=int(H*math.sqrt(1-lam)); cx,cy=random.randint(0,W-1),random.randint(0,H-1)\n    x1,y1=max(cx-cut_w//2,0),max(cy-cut_h//2,0); x2,y2=min(cx+cut_w//2,W),min(cy+cut_h//2,H); return x1,y1,x2,y2\ndef mixup_data(x, y, alpha=0.2):\n    lam=np.random.beta(alpha,alpha) if alpha>0 else 1.0; idx=torch.randperm(x.size(0),device=x.device)\n    mixed_x=lam*x+(1-lam)*x[idx]; y_a,y_b=y,y[idx]; return mixed_x,y_a,y_b,lam\ndef mixup_criterion(criterion, pred, y_a, y_b, lam): return lam*criterion(pred,y_a)+(1-lam)*criterion(pred,y_b)\n@torch.no_grad()\ndef tta_logits(model,x,device,tta=4):\n    outs=[]\n    for op in range(tta):\n        xx=x;\n        if op%2==1: xx=torch.flip(xx,dims=[-1])\n        with torch.amp.autocast(device_type=device.type, enabled=(device.type==\"cuda\")): outs.append(model(xx))\n    return torch.stack(outs,0).mean(0)\n@torch.no_grad()\ndef evaluate(model,dl,device,pos_index,tta=4):\n    model.eval(); y_true,y_pred,y_prob_pos=[],[],[]; total_loss,n=0.0,0\n    criterion=nn.CrossEntropyLoss()\n    for x,y in dl:\n        x,y=x.to(device,non_blocking=True),y.to(device,non_blocking=True)\n        logits=tta_logits(model,x,device,tta=tta) if tta and tta>1 else model(x)\n        with torch.amp.autocast(device_type=device.type,enabled=(device.type==\"cuda\")): loss=criterion(logits,y)\n        prob=torch.softmax(logits,dim=1)\n        y_true.extend(y.cpu().tolist());y_pred.extend(prob.argmax(1).cpu().tolist());y_prob_pos.extend(prob[:,pos_index].cpu().tolist())\n        bs=y.size(0);total_loss+=loss.item()*bs;n+=bs\n    acc=accuracy_score(y_true,y_pred); prec,rec,f1,_=precision_recall_fscore_support(y_true,y_pred,average=\"binary\",pos_label=pos_index,zero_division=0)\n    try: auc=roc_auc_score([1 if t==pos_index else 0 for t in y_true], y_prob_pos)\n    except Exception: auc=float(\"nan\")\n    cm=confusion_matrix(y_true,y_pred,labels=[0,1]); return (total_loss/n,acc,prec,rec,f1,auc,np.array(cm),np.array(y_true),np.array(y_prob_pos))\ndef plot_curves(history,out_dir:Path):\n    out_dir.mkdir(parents=True,exist_ok=True);epochs=np.arange(1,len(history[\"train_loss\"])+1)\n    plt.figure();plt.plot(epochs,history[\"train_loss\"],label=\"train loss\");plt.plot(epochs,history[\"val_loss\"],label=\"val loss\");plt.xlabel(\"epoch\");plt.ylabel(\"loss\");plt.legend();plt.tight_layout();plt.savefig(out_dir/\"loss_curves.png\");plt.close()\n    plt.figure();plt.plot(epochs,history[\"train_acc\"],label=\"train acc\");plt.plot(epochs,history[\"val_acc\"],label=\"val acc\");plt.xlabel(\"epoch\");plt.ylabel(\"accuracy\");plt.legend();plt.tight_layout();plt.savefig(out_dir/\"acc_curves.png\");plt.close()\ndef plot_cm(cm,out_dir:Path,class_names):\n    plt.figure();plt.imshow(cm,interpolation=\"nearest\");plt.title(\"Confusion Matrix\");plt.colorbar();ticks=np.arange(len(class_names));plt.xticks(ticks,class_names,rotation=45);plt.yticks(ticks,class_names);thresh=cm.max()/2.0\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]): plt.text(j,i,int(cm[i,j]),ha=\"center\",va=\"center\",color=\"white\" if cm[i,j]>thresh else \"black\")\n    plt.ylabel(\"True\");plt.xlabel(\"Predicted\");plt.tight_layout();plt.savefig(out_dir/\"confusion_matrix.png\");plt.close()\ndef plot_roc_and_save(y_true_bin,y_prob_pos,out_dir:Path,pos_label_name=\"fake\"):\n    fpr,tpr,_=roc_curve(y_true_bin,y_prob_pos);auc=roc_auc_score(y_true_bin,y_prob_pos)\n    plt.figure();plt.plot(fpr,tpr,label=f\"AUC={auc:.3f}\");plt.plot([0,1],[0,1],\"--\");plt.xlabel(\"FPR\");plt.ylabel(\"TPR\");plt.title(f\"ROC ({pos_label_name} positive)\");plt.legend();plt.tight_layout();plt.savefig(out_dir/\"roc_curve.png\");plt.close();return auc\n\n# ----------------------- Main -----------------------\ndef main():\n    \n    # =================================================================================\n    # ===> KAGGLE USER CONFIGURATION SECTION (CRITICAL!) <===\n    USER_IMAGE_DATA_ROOT = \"/kaggle/input/faceforensics/WD_subset_png\"\n    USER_VIDEO_DATA_ROOT = \"/kaggle/input/wilddeepfake/ffpp_subset_c23\"\n    USER_OUT_DIR = \"/kaggle/working/training_results\"\n\n    # ===> NEW: RESUME TRAINING CONFIGURATION <===\n    RESUME_TRAINING = True # SET THIS TO True TO RESUME\n    # This path points to the 'latest' checkpoint inside your output directory\n    CHECKPOINT_PATH = f\"{USER_OUT_DIR}/checkpoints/resnet50_epoch80.pt\"\n    # =================================================================================\n\n    args_dict = {\n        \"image_data_root\": USER_IMAGE_DATA_ROOT, \"video_data_root\": USER_VIDEO_DATA_ROOT,\n        \"out_dir\": USER_OUT_DIR, \"per_seq_cap\": 60, \n        \"epochs\": 90, # <== INCREASE THE TOTAL NUMBER OF EPOCHS\n        \"batch_size\": 128, \"img_size\": 224, \"workers\": 2,\n        \"lr\": 3e-4, \"weight_decay\": 1e-4, \"label_smoothing\": 0.05, \"freeze_backbone\": False,\n        \"class_weights\": True, \"mixup_p\": 0.3, \"cutmix_p\": 0.2, \"mix_alpha\": 0.2,\n        \"ema_decay\": 0.999, \"grad_clip\": 1.0, \"warmup_pct\": 0.1, \"tta\": 4, \"seed\": 42,\n    }\n    args = argparse.Namespace(**args_dict)\n\n    print(\"\\n[Kaggle Setup]\");\n    if not torch.cuda.is_available(): print(\"\\n!!!! WARNING: GPU ACCELERATOR NOT DETECTED !!!!\\n\")\n    else: print(f\"GPU Detected: {torch.cuda.get_device_name(0)}\")\n\n    set_seed(args.seed); device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); out_dir = Path(args.out_dir)\n    ckpt_dir = out_dir / \"checkpoints\"; art_dir = out_dir / \"artifacts\"\n    ckpt_dir.mkdir(parents=True, exist_ok=True); art_dir.mkdir(parents=True, exist_ok=True)\n    print(f\"\\nSaving outputs to: {out_dir}\")\n\n    train_ds, val_ds, train_dl, val_dl = build_loaders(\n        image_data_root=args.image_data_root, video_data_root=args.video_data_root,\n        img_size=args.img_size, batch_size=args.batch_size, workers=args.workers, per_seq_cap=args.per_seq_cap\n    )\n    print(f\"Master Classes: {train_ds.class_to_idx}\"); idx_fake = train_ds.class_to_idx.get(\"fake\", 0)\n\n    class_weights_tensor = None\n    if args.class_weights:\n        print(\"Computing class weights for combined dataset...\")\n        sampler_indices = list(train_dl.sampler.indices); labels = [train_ds.targets[i] for i in sampler_indices]\n        counts = np.bincount(labels, minlength=len(train_ds.classes)); total = counts.sum()\n        weights = [total/(len(counts)*c) if c > 0 else 0.0 for c in counts]\n        class_weights_tensor = torch.tensor(weights, dtype=torch.float32, device=device)\n        print(f\"Class counts in one epoch: {counts}. Weights: {class_weights_tensor.cpu().numpy()}\")\n\n    print(\"\\nInitializing ResNet-50 model...\"); model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n    model.fc = nn.Linear(model.fc.in_features, len(train_ds.classes))\n    if args.freeze_backbone: [p.requires_grad_(False) for n, p in model.named_parameters() if not n.startswith(\"fc.\")]\n    model.to(device)\n    \n    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=args.label_smoothing)\n    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, weight_decay=args.weight_decay)\n    total_steps = len(train_dl) * args.epochs; warmup_steps = int(args.warmup_pct * total_steps)\n    scheduler = WarmupThenCosine(optimizer, warmup_steps=warmup_steps, total_steps=total_steps)\n    scaler = torch.amp.GradScaler(enabled=(device.type==\"cuda\"))\n    ema = EMA(model, decay=args.ema_decay)\n    history = defaultdict(list); best_auc, best_epoch = -1.0, -1; global_step = 0; start_epoch = 1\n\n    # ===> NEW: LOGIC TO LOAD CHECKPOINT AND RESUME <===\n    if RESUME_TRAINING:\n        print(\"\\n[Resuming Training]\")\n        if not Path(CHECKPOINT_PATH).exists():\n            print(f\"  !!! WARNING: Checkpoint not found at {CHECKPOINT_PATH}. Starting from scratch. !!!\")\n        else:\n            # For modern PyTorch, weights_only=False is needed for non-tensor data\n            ckpt = torch.load(CHECKPOINT_PATH, map_location='cpu', weights_only=False)\n            \n            # The saved model is the EMA model\n            model.load_state_dict(ckpt['model'], strict=True) \n            model.to(device)\n            ema.shadow = {k: v.to(device) for k, v in ckpt['model'].items()}\n            \n            start_epoch = ckpt.get('epoch', 0) + 1\n            \n            print(f\"  Fast-forwarding LR scheduler to step of epoch {start_epoch - 1}...\")\n            steps_to_advance = (start_epoch - 1) * len(train_dl)\n            for _ in range(steps_to_advance): scheduler.step()\n\n            history_path = out_dir / \"history.json\"\n            if history_path.exists():\n                with open(history_path, 'r') as f: history = defaultdict(list, json.load(f))\n                if history.get('val_auc'): best_auc = max(history.get('val_auc', [0.0]))\n            print(f\"  --> Checkpoint loaded. Resuming from Epoch {start_epoch}. Previous best AUC: {best_auc:.4f}\")\n\n    print(\"\\nStarting Training...\")\n    # The loop now starts from 'start_epoch'\n    for epoch in range(start_epoch, args.epochs + 1):\n        model.train(); tr_loss_sum, tr_correct, n_samples = 0.0, 0, 0\n        pbar = tqdm(train_dl, desc=f\"Epoch {epoch}/{args.epochs}\", leave=False)\n        for x, y in pbar:\n            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            use_mixup = random.random() < args.mixup_p; use_cutmix = (not use_mixup) and (random.random() < args.cutmix_p)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.amp.autocast(device_type=device.type, enabled=(device.type==\"cuda\")):\n                if use_mixup:\n                    x_mix, y_a, y_b, lam = mixup_data(x, y, alpha=args.mix_alpha)\n                    logits = model(x_mix); loss = mixup_criterion(criterion, logits, y_a, y_b, lam); y_for_acc = y_a\n                elif use_cutmix:\n                    lam = np.random.beta(args.mix_alpha, args.mix_alpha); idx_perm = torch.randperm(x.size(0), device=x.device)\n                    y_a, y_b = y, y[idx_perm]; W, H = x.shape[3], x.shape[2]; x1, y1, x2, y2 = rand_bbox(W, H, lam)\n                    x[:, :, y1:y2, x1:x2] = x[idx_perm, :, y1:y2, x1:x2]; lam_adj = 1 - ((x2 - x1) * (y2 - y1) / (W * H))\n                    logits = model(x); loss = mixup_criterion(criterion, logits, y_a, y_b, lam_adj); y_for_acc = y_a\n                else: logits = model(x); loss = criterion(logits, y); y_for_acc = y\n            scaler.scale(loss).backward(); scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n            scaler.step(optimizer); scaler.update(); ema.update(model); scheduler.step(); global_step += 1\n            bs = y.size(0); tr_loss_sum += loss.item() * bs; tr_correct += (logits.detach().argmax(1) == y_for_acc).sum().item(); n_samples += bs\n            pbar.set_postfix(loss=tr_loss_sum/max(1,n_samples), acc=tr_correct/max(1,n_samples))\n        tr_loss = tr_loss_sum / max(1, n_samples); tr_acc  = tr_correct / max(1, n_samples)\n        state_backup = {k: v.detach().clone() for k, v in model.state_dict().items()}\n        model.load_state_dict(ema.shadow, strict=True)\n        val_loss, val_acc, prec, rec, f1, val_auc, cm, y_true, y_prob_pos = evaluate(model, val_dl, device, pos_index=idx_fake, tta=args.tta)\n        model.load_state_dict(state_backup, strict=True)\n        \n        # We now save optimizer and scheduler states for even better resumption next time\n        ckpt = {\n            \"model\": ema.shadow, \"epoch\": epoch, \"val_auc\": float(val_auc), \n            \"class_to_idx\": train_ds.class_to_idx, \"args\": args_dict,\n            \"optimizer\": optimizer.state_dict(), \"scheduler\": scheduler.state_dict()\n        }\n        torch.save(ckpt, ckpt_dir / f\"resnet50_epoch{epoch:02d}.pt\")\n        # Overwrite a 'latest' file for easy resuming\n        torch.save(ckpt, ckpt_dir / \"resnet50_latest.pt\")\n        \n        history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc); history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc); history[\"val_auc\"].append(val_auc); history[\"val_f1\"].append(f1)\n        tqdm.write(f\"Epoch {epoch:02d} | tr Ls {tr_loss:.4f} Ac {tr_acc:.4f} | val Ls {val_loss:.4f} Ac {val_acc:.4f} AUC {val_auc:.4f} F1 {f1:.4f}\")\n        class_names = [k for k,_ in sorted(train_ds.class_to_idx.items(), key=lambda kv: kv[1])]\n        plot_cm(cm, art_dir, class_names=class_names)\n        y_true_bin = (y_true == idx_fake).astype(int)\n        _ = plot_roc_and_save(y_true_bin, y_prob_pos, art_dir, pos_label_name=\"fake\")\n        if not math.isnan(val_auc) and val_auc > best_auc:\n            best_auc, best_epoch = val_auc, epoch; torch.save(ckpt, ckpt_dir/\"resnet50_best.pt\")\n            tqdm.write(f\"  >>> New Best AUC: {best_auc:.4f} saved.\")\n    plot_curves(history, art_dir);\n    with open(out_dir/\"history.json\",\"w\") as f: json.dump(history, f, indent=2)\n    print(f\"\\nTraining Complete.\\nBest epoch by AUC: {best_epoch} (AUC={best_auc:.4f})\\nCheckpoints: {ckpt_dir}\\nArtifacts: {art_dir}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T09:34:37.187027Z","iopub.execute_input":"2025-10-11T09:34:37.187872Z","iopub.status.idle":"2025-10-11T09:54:08.848220Z","shell.execute_reply.started":"2025-10-11T09:34:37.187844Z","shell.execute_reply":"2025-10-11T09:54:08.847310Z"}},"outputs":[{"name":"stdout","text":"\n[Kaggle Setup]\nGPU Detected: Tesla T4\nSeed set to 42.\n\nSaving outputs to: /kaggle/working/training_results\nLoading IMAGE data from: /kaggle/input/faceforensics/WD_subset_png\nLoading VIDEO data from: /kaggle/input/wilddeepfake/ffpp_subset_c23\n\nCombined 2 dataset(s). Training samples: 47927, Validation samples: 5219\nBuilding sequence-limited sampler for combined dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Indexing sequences:   0%|          | 0/47927 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b530cc8ef584f2eaf0abe217d5d0e97"}},"metadata":{}},{"name":"stdout","text":"Master Classes: {'fake': 0, 'real': 1}\nComputing class weights for combined dataset...\nClass counts in one epoch: [1211 1530]. Weights: [1.1317093  0.89575166]\n\nInitializing ResNet-50 model...\n\n[Resuming Training]\n  Fast-forwarding LR scheduler to step of epoch 80...\n  --> Checkpoint loaded. Resuming from Epoch 81. Previous best AUC: 0.9306\n\nStarting Training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 81/90:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 81 | tr Ls 0.2474 Ac 0.8760 | val Ls 0.3791 Ac 0.8335 AUC 0.9295 F1 0.8211\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 82/90:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 82 | tr Ls 0.1672 Ac 0.8606 | val Ls 0.3765 Ac 0.8373 AUC 0.9296 F1 0.8241\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 83/90:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 83 | tr Ls 0.1798 Ac 0.8840 | val Ls 0.3737 Ac 0.8396 AUC 0.9299 F1 0.8260\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 84/90:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 84 | tr Ls 0.2044 Ac 0.9602 | val Ls 0.3711 Ac 0.8429 AUC 0.9301 F1 0.8288\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 85/90:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 85 | tr Ls 0.1642 Ac 0.8916 | val Ls 0.3689 Ac 0.8444 AUC 0.9304 F1 0.8299\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 86/90:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 86 | tr Ls 0.1945 Ac 0.9325 | val Ls 0.3666 Ac 0.8461 AUC 0.9306 F1 0.8313\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 87/90:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 87 | tr Ls 0.1869 Ac 0.8916 | val Ls 0.3644 Ac 0.8481 AUC 0.9308 F1 0.8327\n  >>> New Best AUC: 0.9308 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 88/90:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 88 | tr Ls 0.1874 Ac 0.9019 | val Ls 0.3624 Ac 0.8498 AUC 0.9308 F1 0.8338\n  >>> New Best AUC: 0.9308 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 89/90:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 89 | tr Ls 0.1947 Ac 0.9449 | val Ls 0.3602 Ac 0.8509 AUC 0.9311 F1 0.8344\n  >>> New Best AUC: 0.9311 saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 90/90:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 90 | tr Ls 0.1734 Ac 0.9369 | val Ls 0.3584 Ac 0.8527 AUC 0.9312 F1 0.8359\n  >>> New Best AUC: 0.9312 saved.\n\nTraining Complete.\nBest epoch by AUC: 90 (AUC=0.9312)\nCheckpoints: /kaggle/working/training_results/checkpoints\nArtifacts: /kaggle/working/training_results/artifacts\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os, json, zipfile\nfrom pathlib import Path\n\nOUT = Path(\"/kaggle/working/training_results\")\nCKPT = OUT / \"checkpoints\" / \"resnet50_best.pt\"\nART = OUT / \"artifacts\"\nHIST = OUT / \"history.json\"\nZIP_PATH = Path(\"/kaggle/working/resnet50_mixed_export.zip\")\n\nassert CKPT.exists(), f\"Best checkpoint not found: {CKPT}\"\n\n# Also save a light state_dict-only file for easy loading\nimport torch\nbest = torch.load(CKPT, map_location=\"cpu\")\nstate_dict = best[\"model\"]\ntorch.save(state_dict, OUT / \"resnet50_streamlit_state_dict.pth\")\n\n# Pack ZIP\nwith zipfile.ZipFile(ZIP_PATH, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n    z.write(CKPT, arcname=\"resnet50_best.pt\")\n    if (OUT/\"resnet50_streamlit_state_dict.pth\").exists():\n        z.write(OUT/\"resnet50_streamlit_state_dict.pth\", arcname=\"resnet50_streamlit_state_dict.pth\")\n    if HIST.exists():\n        z.write(HIST, arcname=\"history.json\")\n    if ART.exists():\n        for p in ART.glob(\"*\"):\n            z.write(p, arcname=f\"artifacts/{p.name}\")\n\nprint(f\"Ready: {ZIP_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T10:08:03.483191Z","iopub.execute_input":"2025-10-11T10:08:03.483978Z","iopub.status.idle":"2025-10-11T10:08:22.336376Z","shell.execute_reply.started":"2025-10-11T10:08:03.483951Z","shell.execute_reply":"2025-10-11T10:08:22.335549Z"}},"outputs":[{"name":"stdout","text":"Ready: /kaggle/working/resnet50_mixed_export.zip\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}